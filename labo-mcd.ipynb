{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\narchivos = []\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        archivos.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-02T00:04:02.107796Z","iopub.execute_input":"2023-05-02T00:04:02.108469Z","iopub.status.idle":"2023-05-02T00:04:02.127782Z","shell.execute_reply.started":"2023-05-02T00:04:02.108425Z","shell.execute_reply":"2023-05-02T00:04:02.126751Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dset-pequenio/DiccionarioDatos_2023.ods\n/kaggle/input/dset-peq/dataset_pequeno.csv\n/kaggle/input/laboratorio-de-imp-i-2023-virtual/kaggle_competencia_muestra.csv\n/kaggle/input/lab1-complete-parquet/downsampled_df.csv\n/kaggle/input/lab1-complete-parquet/downsampled_df.parquet\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\nimport os\n\n\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom featurewiz import featurewiz\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom scipy import sparse\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 200)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:04:02.133885Z","iopub.execute_input":"2023-05-02T00:04:02.134579Z","iopub.status.idle":"2023-05-02T00:04:04.000206Z","shell.execute_reply.started":"2023-05-02T00:04:02.134539Z","shell.execute_reply":"2023-05-02T00:04:03.998898Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stdout","text":"Imported 0.3.0 version. Select nrows to a small number when running on huge datasets.\noutput = featurewiz(dataname, target, corr_limit=0.90, verbose=2, sep=',', \n\t\theader=0, test_data='',feature_engg='', category_encoders='',\n\t\tdask_xgboost_flag=False, nrows=None, skip_sulov=False)\nCreate new features via 'feature_engg' flag : ['interactions','groupby','target']\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read the dataset\ndataset = pd.read_csv(archivos[3])\nprint(archivos[3])\n\ndataset['clase_ternaria'].replace({'BAJA+2': 1, 'BAJA+1':1, 'CONTINUA':0}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:04:04.001826Z","iopub.execute_input":"2023-05-02T00:04:04.002517Z","iopub.status.idle":"2023-05-02T00:04:05.295291Z","shell.execute_reply.started":"2023-05-02T00:04:04.002476Z","shell.execute_reply":"2023-05-02T00:04:05.294163Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/lab1-complete-parquet/downsampled_df.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def find_cols_with_high_nan(df, threshold=0.80, drop=True): \n    nan_pct = df.isna().sum() / len(df)    \n    # Select columns with NaN percentage greater than the threshold\n    high_nan_cols = nan_pct[nan_pct > threshold].index.tolist()\n    if drop:\n        print(f'dropeando {high_nan_cols} porque tienen mas de {threshold} Nan...')\n        return df.drop(columns=high_nan_cols)\n    return high_nan_cols\n\ndef join_visa_master(df):\n    print('juntando columnas con prefijo Visa_ y Master_')\n    visa_columns = df.filter(like='Visa_').columns.to_list()\n    master_columns = df.filter(like='Master_').columns.to_list()\n    \n    for col_visa, col_master in zip(visa_columns, master_columns):\n        new_column_name = 'sum_' + col_visa.split('_', 1)[1]\n        \n        # Replace NaN values with 0 temporarily for the sum\n        visa_values = df[col_visa].fillna(0)\n        master_values = df[col_master].fillna(0)\n        \n        # Sum the non-NaN values and store the result in the new column\n        df[new_column_name] = visa_values + master_values\n        df[new_column_name] = df[new_column_name].replace(0, np.nan)\n    \n    return df.drop(columns=visa_columns+master_columns)\n\n\ndef join_cols(df, string):\n    joined_cols = df.filter(like=string)\n    print(f'sumando las columnas: {joined_cols.columns.to_list()}...')\n    \n    # Replace NaN values with 0 temporarily for the sum\n    joined_cols_filled = joined_cols.fillna(0)\n    \n    # Calculate the sum\n    sum_values = joined_cols_filled.sum(axis=1)\n    \n    # Replace 0 values with NaN after the summation, but keep the original 0 values when both columns have 0 values in the same row\n    df[string] = sum_values.where((joined_cols_filled != 0).any(axis=1), joined_cols_filled.sum(axis=1))\n    \n    return df.drop(columns=joined_cols.columns)\n\ndef find_good_ratios(data):\n    '''crea ratios de todas las features float y se queda con los que featurewiz le da importancia, devuelve una lista de buenos candidatos para ratio'''\n    def _create_ratios(dataframe, variable,target):\n        float_cols = dataframe.select_dtypes(include='float64').columns.tolist()    \n        new_dataframe = pd.DataFrame()    \n        for col in float_cols:\n            if col != variable:\n                new_col_name = f'{variable}_to_{col}'\n                new_dataframe[new_col_name] = dataframe[variable] / dataframe[col]\n                new_dataframe.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n        new_dataframe[target] = dataframe[target]\n        return new_dataframe    \n    \n    the_best = []\n    for i in data.select_dtypes(include='float64').columns.tolist():\n        temp_df = _create_ratios(data, i,'clase_ternaria')\n        try:\n            best_ratios = featurewiz(temp_df, 'clase_ternaria')\n            the_best.append(best_ratios[0])\n        except:\n            the_best.append(f'error with {i}')\n    return the_best\n\ndef create_ratios_from_best(df, lista_best):\n    '''crea las features a partir de una lista de listas que tiene los mejores ratios'''\n    data = df.copy()    \n    for ratios in lista_best:\n        if \"error\" in ratios:\n            print(f\"Skipping invalid ratio: {ratios}\")\n            continue\n        else:\n            for j in ratios:\n                variables = j.split('_to_')\n                data[j] = data[variables[0]] / data[variables[1]]           \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:04:05.298776Z","iopub.execute_input":"2023-05-02T00:04:05.299067Z","iopub.status.idle":"2023-05-02T00:04:05.315906Z","shell.execute_reply.started":"2023-05-02T00:04:05.299038Z","shell.execute_reply":"2023-05-02T00:04:05.314848Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset = find_cols_with_high_nan(dataset)\ndataset = join_visa_master(dataset)\n\npalabras_agrupar =['prestamo','seguro','servicios', 'comisiones','cheques','ahorro','inversion','_consumo','descuentos','tarjeta','consumo','margen','debit','forex', 'transfer','autoservicio','cajas','atm','mobile']\nfor i in palabras_agrupar:\n    dataset = join_cols(dataset, i)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:04:06.407556Z","iopub.execute_input":"2023-05-02T00:04:06.408523Z","iopub.status.idle":"2023-05-02T00:04:07.698558Z","shell.execute_reply.started":"2023-05-02T00:04:06.408466Z","shell.execute_reply":"2023-05-02T00:04:07.697322Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"dropeando ['Master_Finiciomora', 'Visa_Finiciomora'] porque tienen mas de 0.8 Nan...\njuntando columnas con prefijo Visa_ y Master_\nsumando las columnas: ['cprestamos_personales', 'mprestamos_personales', 'cprestamos_prendarios', 'mprestamos_prendarios', 'cprestamos_hipotecarios', 'mprestamos_hipotecarios']...\nsumando las columnas: ['cseguro_vida', 'cseguro_auto', 'cseguro_vivienda', 'cseguro_accidentes_personales']...\nsumando las columnas: ['cpagodeservicios', 'mpagodeservicios']...\nsumando las columnas: ['mcomisiones', 'ccomisiones_mantenimiento', 'mcomisiones_mantenimiento', 'ccomisiones_otras', 'mcomisiones_otras']...\nsumando las columnas: ['ccheques_depositados', 'mcheques_depositados', 'ccheques_emitidos', 'mcheques_emitidos', 'ccheques_depositados_rechazados', 'mcheques_depositados_rechazados', 'ccheques_emitidos_rechazados', 'mcheques_emitidos_rechazados']...\nsumando las columnas: ['ccaja_ahorro', 'mcaja_ahorro', 'mcaja_ahorro_adicional', 'mcaja_ahorro_dolares']...\nsumando las columnas: ['cinversion1', 'minversion1_pesos', 'minversion1_dolares', 'cinversion2', 'minversion2']...\nsumando las columnas: ['mtarjeta_visa_consumo', 'mtarjeta_master_consumo']...\nsumando las columnas: ['ccajeros_propios_descuentos', 'mcajeros_propios_descuentos', 'ctarjeta_visa_descuentos', 'mtarjeta_visa_descuentos', 'ctarjeta_master_descuentos', 'mtarjeta_master_descuentos']...\nsumando las columnas: ['ctarjeta_debito', 'ctarjeta_debito_transacciones', 'ctarjeta_visa', 'ctarjeta_visa_transacciones', 'ctarjeta_master', 'ctarjeta_master_transacciones', 'ctarjeta_visa_debitos_automaticos', 'mttarjeta_visa_debitos_automaticos', 'ctarjeta_master_debitos_automaticos', 'mttarjeta_master_debitos_automaticos']...\nsumando las columnas: ['sum_mconsumospesos', 'sum_mconsumosdolares', 'sum_mconsumototal', 'sum_cconsumos', '_consumo']...\nsumando las columnas: ['mactivos_margen', 'mpasivos_margen']...\nsumando las columnas: ['ccuenta_debitos_automaticos', 'mcuenta_debitos_automaticos']...\nsumando las columnas: ['cforex', 'cforex_buy', 'mforex_buy', 'cforex_sell', 'mforex_sell']...\nsumando las columnas: ['ctransferencias_recibidas', 'mtransferencias_recibidas', 'ctransferencias_emitidas', 'mtransferencias_emitidas']...\nsumando las columnas: ['mautoservicio', 'cextraccion_autoservicio', 'mextraccion_autoservicio']...\nsumando las columnas: ['ccajas_transacciones', 'ccajas_consultas', 'ccajas_depositos', 'ccajas_extracciones', 'ccajas_otras']...\nsumando las columnas: ['catm_trx', 'matm', 'catm_trx_other', 'matm_other']...\nsumando las columnas: ['tmobile_app', 'cmobile_app_trx']...\n","output_type":"stream"}]},{"cell_type":"code","source":"list_of_best_ratios = find_good_ratios(dataset)\ndataset = create_ratios_from_best(dataset, list_of_best_ratios)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lista_floats = dataset.select_dtypes(include='float64').columns.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _create_ratios(dataframe, variable,target):\n    float_cols = dataframe.select_dtypes(include='float64').columns.tolist()    \n    new_dataframe = pd.DataFrame()    \n    for col in float_cols:\n        if col != variable:\n            new_col_name = f'{variable}_to_{col}'\n            new_dataframe[new_col_name] = dataframe[variable] / dataframe[col]\n            new_dataframe.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    new_dataframe[target] = dataframe[target]\n    return new_dataframe ","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:04:12.797877Z","iopub.execute_input":"2023-05-02T00:04:12.798818Z","iopub.status.idle":"2023-05-02T00:04:12.805593Z","shell.execute_reply.started":"2023-05-02T00:04:12.798763Z","shell.execute_reply":"2023-05-02T00:04:12.804511Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"temp_df = _create_ratios(dataset, 'mrentabilidad','clase_ternaria')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:04:13.585789Z","iopub.execute_input":"2023-05-02T00:04:13.586608Z","iopub.status.idle":"2023-05-02T00:04:14.124609Z","shell.execute_reply.started":"2023-05-02T00:04:13.586567Z","shell.execute_reply":"2023-05-02T00:04:14.123495Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'eta': 0.01, 'subsample': 0.5, 'grow_policy': 'depthwise'}\nfeaturewiz(dataname=temp_df, target='clase_ternaria', corr_limit=0.8, verbose=True, test_data=None, feature_engg=None, category_encoders=None, dask_xgboost_flag=False, nrows=None, skip_sulov=False, **params)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:05:44.774434Z","iopub.execute_input":"2023-05-02T00:05:44.775227Z","iopub.status.idle":"2023-05-02T00:05:44.869597Z","shell.execute_reply.started":"2023-05-02T00:05:44.775180Z","shell.execute_reply":"2023-05-02T00:05:44.868105Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nYou supplied eta = 0.01\nYou supplied subsample = 0.5\nYou supplied grow_policy = depthwise\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Binary_Classification problem ####\n    Loaded train data. Shape = (86870, 43)\n#### Single_Label Binary_Classification problem ####\nloading the entire test dataframe - there is no nrows limit applicable #########\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3086/700271202.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'eta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subsample'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grow_policy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'depthwise'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeaturewiz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'clase_ternaria'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_engg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_encoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdask_xgboost_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_sulov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/featurewiz/featurewiz.py\u001b[0m in \u001b[0;36mfeaturewiz\u001b[0;34m(dataname, target, corr_limit, verbose, sep, header, test_data, feature_engg, category_encoders, dask_xgboost_flag, nrows, skip_sulov, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading the entire test dataframe - there is no nrows limit applicable #########'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         test_data = load_file_dataframe(test_data, sep=sep, header=header, \n\u001b[0;32m--> 648\u001b[0;31m                         verbose=verbose, nrows=None, target=settings.modeltype, is_test_flag=True)\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;31m### sometimes, test_data returns None if there is an error. ##########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/featurewiz/featurewiz.py\u001b[0m in \u001b[0;36mload_file_dataframe\u001b[0;34m(dataname, sep, header, verbose, nrows, parse_dates, target, is_test_flag)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 dfte, _ = train_test_split(dfte, test_size=test_size, stratify=dfte[target],\n\u001b[1;32m    273\u001b[0m                                 shuffle=True, random_state=99)\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdfte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You have duplicate column names in your data set. Removing duplicate columns now...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mdfte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfte\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdfte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"],"ename":"TypeError","evalue":"'NoneType' object is not iterable","output_type":"error"}]},{"cell_type":"code","source":"featurewiz(temp_df.fillna(0), 'clase_ternaria')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T00:06:44.583476Z","iopub.execute_input":"2023-05-02T00:06:44.584613Z","iopub.status.idle":"2023-05-02T00:06:45.335769Z","shell.execute_reply.started":"2023-05-02T00:06:44.584573Z","shell.execute_reply":"2023-05-02T00:06:45.333064Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"featurewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Binary_Classification problem ####\n    Loaded train data. Shape = (86870, 43)\n#### Single_Label Binary_Classification problem ####\nNo test data filename given...\nClassifying features using a random sample of 10000 rows from dataset...\n#### Single_Label Binary_Classification problem ####\n    loading a random sample of 10000 rows into pandas for EDA\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3086/2114126199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeaturewiz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clase_ternaria'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/featurewiz/featurewiz.py\u001b[0m in \u001b[0;36mfeaturewiz\u001b[0;34m(dataname, target, corr_limit, verbose, sep, header, test_data, feature_engg, category_encoders, dask_xgboost_flag, nrows, skip_sulov, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nthread'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tree_method'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpu_hist'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eta'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subsample'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grow_policy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'depthwise'\u001b[0m \u001b[0;31m# 'lossguide' #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'params' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'params' referenced before assignment","output_type":"error"}]},{"cell_type":"code","source":"temp_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_keep_as_is = ['active_quarter','cliente_vip','internet','tcuentas','cdescubierto_preacordado','ccaja_seguridad','tcallcenter','thomebanking','cplazo_fijo','cajas','mobile']\ncols_to_binarize = ['cliente_edad','cliente_antiguedad','cproductos','cpagomiscuentas','ccajeros_propios_descuentos','ccallcenter_transacciones','chomebanking_transacciones','seguro','trx']\ncols_to_normalize = list(dtrain.select_dtypes('float').columns)\n\n#len(cols_keep_as_is + cols_to_binarize+cols_to_normalize) == len(set(cols_keep_as_is + cols_to_binarize+cols_to_normalize))\nset(dtrain.columns) - set(cols_keep_as_is + cols_to_binarize+cols_to_normalize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pipeline preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion    \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.validation import check_is_fitted\n\n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.columns]\n\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)\n    \n    \nclass PandasFeatureUnion(BaseEstimator, TransformerMixin):\n    def __init__(self, transformers):\n        self.transformers = transformers\n\n    def fit(self, X, y=None):\n        for _, transformer in self.transformers:\n            transformer.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        transformed_dfs = []\n        for _, transformer in self.transformers:\n            transformed_dfs.append(transformer.transform(X))        \n        \n        concatenated_df = pd.concat(transformed_dfs, axis=1)\n        return concatenated_df\n    \n\nclass BinTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins, columns=None):\n        self.n_bins = n_bins\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        self.quantiles_ = {}\n        self.outliers_ = {}\n        cols_to_transform = self.columns or X.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:          \n            q1, q3 = np.percentile(X[col], [25, 75])\n            iqr = q3 - q1\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n            mask = (X[col] < lower_bound) | (X[col] > upper_bound)\n            self.outliers_[col] = mask\n            col_no_outliers = X.loc[~mask, col]\n            self.quantiles_[col] = pd.qcut(col_no_outliers, self.n_bins, labels=False, duplicates='drop')\n        return self\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X_trans = X.copy()\n        cols_to_transform = self.columns or X_trans.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:         \n            X_trans[col] = pd.cut(X_trans[col], bins=self.n_bins, labels=False, duplicates='drop')\n            X_trans.loc[self.outliers_[col].reindex(X.index, fill_value=False).values, col] = 'outlier'\n            if col in self.quantiles_:\n                X_trans.loc[~self.outliers_[col].reindex(X.index, fill_value=False), col] = 'bin_' + self.quantiles_[col].apply(lambda x: str(x)).astype(str)      \n    \n        return X_trans[cols_to_transform]\n    \n    \nclass OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, categorical_cols=None):\n        self.categorical_cols = categorical_cols\n    \n    def fit(self, X, y=None):\n        if self.categorical_cols is None:\n            self.categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        self.categories_ = [X[col].astype('category').cat.categories.tolist() + ['dummy'] for col in self.categorical_cols]\n        self.encoder = OneHotEncoder(categories=self.categories_, handle_unknown='ignore')\n        self.encoder.fit(X[self.categorical_cols])\n        return self\n    \n    def transform(self, X, y=None):\n        X_transformed = self.encoder.transform(X[self.categorical_cols])\n        feature_names = self.encoder.get_feature_names_out(self.categorical_cols)\n        X_transformed_df = pd.DataFrame.sparse.from_spmatrix(X_transformed, columns=feature_names)\n        X_transformed_df.index = X.index\n        return pd.concat([X.drop(columns=self.categorical_cols), X_transformed_df], axis=1)\n\nimport category_encoders as ce\nclass TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, cols=None, drop_invariant=False, return_df=True):\n        self.cols = cols\n        self.drop_invariant = drop_invariant\n        self.return_df = return_df\n        self.encoder = ce.TargetEncoder(cols=self.cols, drop_invariant=self.drop_invariant, return_df=self.return_df)\n\n    def fit(self, X, y=None):\n        self.encoder.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        return self.encoder.transform(X, y)\n    \n    \n\nfrom sklearn.impute import SimpleImputer\n\nclass SimpleImputerWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=strategy)\n    \n    def fit(self, X, y=None):\n        self.imputer.fit(X)\n        return self\n    \n    def transform(self, X):\n        X_transformed = self.imputer.transform(X)\n        return pd.DataFrame(X_transformed, columns=X.columns, index=X.index)\n    \n    \nclass PercentageVariationTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, variable):\n        self.variable = variable\n\n    def fit(self, X, y):\n        # Combine X and y for easier calculations\n        data = pd.concat([X, y], axis=1)\n\n        # Calculate the mean of the variable for different target values\n        self.target_means = data.groupby(y.name)[self.variable].mean()\n        return self\n\n    def transform(self, X, y=None):\n        result = pd.DataFrame()\n\n        # Calculate the percentage variation for each target mean\n        for target_value, target_mean in self.target_means.items():\n            column_name = f\"{self.variable}_pct_variation_{target_value}\"\n            result[column_name] = (X[self.variable] - target_mean) / target_mean * 100\n\n        return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## feature engineering","metadata":{}},{"cell_type":"code","source":"class RatioFeature(BaseEstimator, TransformerMixin):\n    def __init__(self, base_var, other_vars):\n        self.base_var = base_var\n        self.other_vars = other_vars if other_vars is not None else []\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Verificar que base_var existe en el DataFrame\n        if self.base_var not in X.columns:\n            raise ValueError(f\"La columna '{self.base_var}' no se encuentra en el DataFrame\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas features\n        new_features = pd.DataFrame()\n\n        # Calcular las nuevas features\n        for var in self.other_vars:\n            if var in X.columns:\n                feature_name = f\"{self.base_var}_{var}_ratio\"\n                new_features[feature_name] = X[self.base_var] / X[var]\n            else:\n                raise ValueError(f\"La columna '{var}' no se encuentra en el DataFrame\")\n        return new_features\n    \n    \nclass JoinColumnsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, like_strings):\n        self.like_strings = like_strings\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas columnas\n        new_cols = pd.DataFrame(index=X.index)\n\n        # Iterar sobre las cadenas especificadas\n        for like_string in self.like_strings:\n            # Filtrar las columnas que contienen la cadena específica\n            joined_cols = X.filter(like=like_string)\n\n            # Crear una nueva columna con la suma de las columnas filtradas\n            new_col_name = 'sum_'+like_string\n            new_cols[new_col_name] = joined_cols.sum(axis=1)\n\n        return new_cols\n    \nfrom sklearn.preprocessing import StandardScaler\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)\n    \n    \nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport pandas as pd\nimport numpy as np\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ReplaceInfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, factor = 3):\n        self.column_max_values = {}\n        self.column_min_values = {}\n        self.factor = factor\n\n    def fit(self, X, y=None):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input should be a pandas DataFrame\")\n\n        for column in X.columns:\n            max_value = X[column].replace([np.inf, -np.inf], np.nan).max()\n            min_value = X[column].replace([np.inf, -np.inf], np.nan).min()\n\n            self.column_max_values[column] = max_value\n            self.column_min_values[column] = min_value\n\n        return self\n\n    def transform(self, X, y=None):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input should be a pandas DataFrame\")\n\n        X_copy = X.copy()\n\n        for column in X_copy.columns:\n            if column in self.column_max_values:\n                X_copy[column] = X_copy[column].replace(np.inf, self.column_max_values[column] * self.factor)\n            if column in self.column_min_values:\n                X_copy[column] = X_copy[column].replace(-np.inf, self.column_min_values[column] * self.factor)\n\n        return X_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binning = Pipeline([\n    ('col_select',ColumnSelector(columns=cols_to_binarize)),\n    ('bin_convert',BinTransformer(columns=None, n_bins = 3)),\n    ('onehot', OneHotEncoderTransformer())\n    #('target_encoding', TargetEncoderWrapper()) \n])\n\nstandarize = Pipeline([\n    ('col_select',ColumnSelector(columns=cols_to_normalize)),\n    ('scale', StandardScalerTransformer(columns=cols_to_normalize)),\n])\n\nfeature_engineering = PandasFeatureUnion([\n    ('as_is', ColumnSelector(columns=cols_keep_as_is)),\n    ('log', standarize),\n    ('bin_convert',binning)\n])\n\npreprocessing = Pipeline([    \n    ('inf', ReplaceInfTransformer()),\n    ('fillna', SimpleImputerWrapper(strategy='mean')),\n    ('feature_engineering', feature_engineering)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dtrain.drop('clase_ternaria', axis=1)\ny = dtrain['clase_ternaria']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the preprocessing pipeline on the training set and transform both the training and test sets\nX_train_processed = preprocessing.fit_transform(X_train, y_train)\nX_test_processed = preprocessing.transform(X_test)\n\n# Check the shapes of the processed data\nprint(X_train_processed.shape, X_test_processed.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model evaluation","metadata":{}},{"cell_type":"code","source":"def get_balanced_classes():\n    class_counts = y.value_counts()\n    total_samples = class_counts.sum()\n    class_frequencies = class_counts / total_samples\n    class_weights = 1 / class_frequencies\n    return class_weights.to_dict()\n\nclass_weights_dict = get_balanced_classes()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import early_stopping, log_evaluation\n\ndef objective(trial):\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n\n    f1_scores = []\n    for train_index, val_index in kf.split(X_train_processed):\n        X_tr, X_val = X_train_processed.iloc[train_index], X_train_processed.iloc[val_index]\n        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        params = {\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1.0, log=True),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 128),  # Fix the maximum value for num_leaves\n            \n            'class_weight': class_weights_dict,  \n            #\"objective\": \"multiclass\",\n            #\"metric\": \"multi_logloss\",\n            #\"num_class\": 3,\n            \"verbosity\": -1,\n            \"boosting_type\": \"gbdt\",   \n            'n_jobs': -1,\n#             \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),          \n            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 50),\n#             \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n#             \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n#             \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n#             \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n#             \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 1e-8, 1.0)\n        }\n\n        clf = lgb.LGBMClassifier(device='gpu', **params)\n        \n        try:\n            clf.fit(\n                X_tr, y_tr, \n                eval_set=[(X_val, y_val)], \n                callbacks=[early_stopping(50), lgb.log_evaluation(period=50)]\n            )    \n        \n        except lgb.basic.LightGBMError as e:\n            print(f\"Trial {trial.number} failed with parameters: {params}\")\n            if len(trial.study.trials) > 1:\n                print(f\"Current best parameters: {trial.study.best_params}\")\n            return -float('inf')  # Return very low score to skip this set of hyperparameters\n\n            \n        y_pred = clf.predict(X_val)\n        f1 = f1_score(y_val, y_pred, average='macro')\n        f1_scores.append(f1)\n\n    return sum(f1_scores) / len(f1_scores)\n\n\ndef optimize_lgbm_hyperparameters():\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(f\"Best trial: {study.best_trial.params}\")\n    print(f\"Best F1 score: {study.best_value}\")\n    return study.best_trial.params\n\nbest_params = optimize_lgbm_hyperparameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n# from sklearn.model_selection import validation_curve\n\n# # Choose the hyperparameter you want to analyze\n# hyperparameter = 'learning_rate'\n\n# # Define the range of values for the hyperparameter\n# param_range = np.arange(2, 129, 8)  # Adjust this range as needed\n\n# # Get the validation curve\n# train_scores, test_scores = validation_curve(\n#     lgb.LGBMClassifier(device='gpu', **fixed_params),\n#     X_train_processed,\n#     y_train,\n#     param_name=hyperparameter,\n#     param_range=param_range,\n#     cv=3,\n#     scoring='f1_macro',\n#     n_jobs=-1\n# )\n\n# train_scores_mean = np.mean(train_scores, axis=1)\n# train_scores_std = np.std(train_scores, axis=1)\n# test_scores_mean = np.mean(test_scores, axis=1)\n# test_scores_std = np.std(test_scores, axis=1)\n\n# plt.title(f\"Validation Curve with LightGBM ({hyperparameter})\")\n# plt.xlabel(hyperparameter)\n# plt.ylabel(\"F1 Score\")\n# plt.ylim(0.0, 1.1)\n# lw = 2\n# plt.plot(param_range, train_scores_mean, label=\"Training score\",\n#              color=\"darkorange\", lw=lw)\n# plt.fill_between(param_range, train_scores_mean - train_scores_std,\n#                  train_scores_mean + train_scores_std, alpha=0.2,\n#                  color=\"darkorange\", lw=lw)\n# plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n#              color=\"navy\", lw=lw)\n# plt.fill_between(param_range, test_scores_mean - test_scores_std,\n#                  test_scores_mean + test_scores_std, alpha=0.2,\n#                  color=\"navy\", lw=lw)\n# plt.legend(loc=\"best\")\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params =  {'learning_rate': 0.22524132362500338, 'max_depth': 8, 'num_leaves': 102, 'min_data_in_leaf': 50}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nfrom lightgbm import LGBMClassifier\n\n#class_weight = {0:1, 1:40}\n\nmodel = LGBMClassifier(device='gpu',class_weight = class_weights_dict, **best_params)\n\nmodel.fit(X_train_processed, y_train)\n\ny_pred_proba = model.predict_proba(X_test_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## calculate best threshold & best gain","metadata":{}},{"cell_type":"code","source":"# def calculate_gain(cm, fp=-3000, tp=117000):\n#     return (cm[0][1] *fp + cm[0][2]*fp + cm[1][2] * fp + (cm[2][2]*tp)+(cm[1][1]*tp))\n\n# def find_best_thresholds(y_test, y_pred_proba, step=0.1):\n#     max_gain = float('-inf')\n#     best_thresholds = None\n\n#     threshold_range = np.arange(0.001, 1 + step, step)\n#     for thresholds in itertools.product(threshold_range, repeat=3):\n#         y_pred_adjusted_proba = adjust_proba(y_pred_proba, thresholds)\n#         y_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n#         cm = confusion_matrix(y_test, y_pred_adjusted)\n#         gain = calculate_gain(cm)\n\n#         if gain > max_gain:\n#             max_gain = gain\n#             best_thresholds = thresholds\n\n#     return best_thresholds, max_gain\n\n# def adjust_proba(proba, thresholds):\n#     adjusted_proba = np.zeros_like(proba)\n#     for i in range(proba.shape[1]):\n#         adjusted_proba[:, i] = proba[:, i] / thresholds[i]\n#     return adjusted_proba\n\n\n# def calculate_optimal_threshold(model, X_test, y_test, step=0.1, fp=-3000, tp=117000):    \n\n#     y_pred_proba = model.predict_proba(X_test)\n#     best_thresholds, max_gain = find_best_thresholds(y_test, y_pred_proba, step)\n\n#     # Apply the custom threshold function\n#     y_pred_adjusted_proba = adjust_proba(y_pred_proba, best_thresholds)\n#     y_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n\n#     print(\"Best thresholds:\", best_thresholds)\n#     print(\"Max gain:\", max_gain)    \n\n#     report = classification_report(y_test, y_pred_adjusted)\n#     print(\"Classification report:\\n\", report)\n\n#     cm = confusion_matrix(y_test, y_pred_adjusted)\n#     ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n#     ax.set_xlabel('Predicted labels')\n#     ax.set_ylabel('True labels')\n#     plt.show()\n    \n    \n#     # Calculate SHAP values using TreeExplainer\n#     explainer = shap.TreeExplainer(model)\n#     X_sample = X_test.sample(1000)\n#     shap_values = explainer.shap_values(X_sample)   \n#     positive_class_shap_values = shap_values[1]\n#     shap.summary_plot(positive_class_shap_values, X_sample, feature_names=X_sample.columns)\n\n#     return best_thresholds, y_pred_adjusted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport itertools\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\n\ndef calculate_gain(cm, fp=-3000, tp=117000):\n    return (cm[0][1] * fp + (cm[1][1] * tp))\n\ndef find_best_threshold(y_test, y_pred_proba, step=0.1):\n    max_gain = float('-inf')\n    best_threshold = None\n\n    threshold_range = np.arange(0.001, 1 + step, step)\n    for threshold in threshold_range:\n        y_pred_adjusted = (y_pred_proba[:, 1] > threshold).astype(int)\n        cm = confusion_matrix(y_test, y_pred_adjusted)\n        gain = calculate_gain(cm)\n\n        if gain > max_gain:\n            max_gain = gain\n            best_threshold = threshold\n\n    return best_threshold, max_gain\n\ndef calculate_optimal_threshold(model, X_test, y_test, step=0.1, fp=-3000, tp=117000):    \n\n    y_pred_proba = model.predict_proba(X_test)\n    best_threshold, max_gain = find_best_threshold(y_test, y_pred_proba, step)\n\n    # Apply the custom threshold function\n    y_pred_adjusted = (y_pred_proba[:, 1] > best_threshold).astype(int)\n\n    print(\"Best threshold:\", best_threshold)\n    print(\"Max gain:\", max_gain)    \n\n    report = classification_report(y_test, y_pred_adjusted)\n    print(\"Classification report:\\n\", report)\n\n    cm = confusion_matrix(y_test, y_pred_adjusted)\n    ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    plt.show()\n    \n    # Calculate SHAP values using TreeExplainer\n    explainer = shap.TreeExplainer(model)\n    X_sample = X_test.sample(1000)\n    shap_values = explainer.shap_values(X_sample)   \n    positive_class_shap_values = shap_values[1]\n    shap.summary_plot(positive_class_shap_values, X_sample, feature_names=X_sample.columns)\n\n    return best_threshold, y_pred_adjusted\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_thresholds, y_pred_adjusted = calculate_optimal_threshold(model, X_train_processed, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_thresholds, y_pred_adjusted = calculate_optimal_threshold(model, X_test_processed, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_train_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred = model.predict(X_test_processed)\n\ncm = confusion_matrix(y_train, y_pred)\nax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prediccion","metadata":{}},{"cell_type":"code","source":"dapply = preproc_func(dapply,palabras_agrupar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dapply = create_ratios_from_best(dapply, best_unb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred_result = model.predict_proba(preprocessing.transform(dapply))\n\n# y_pred_adjusted_proba = adjust_proba(y_pred_result, best_thresholds)\n# y_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n\n# # Create a DataFrame for predicted labels using the same index as numero_de_cliente\n# predicted_df = pd.DataFrame(y_pred_adjusted, columns=['Predicted'], index=dapply.index)\n\n# # Combine the numero_de_cliente and predicted labels\n# end_result = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_result = model.predict_proba(preprocessing.transform(dapply))\n\n# Apply the custom threshold function\ny_pred_adjusted = (y_pred_result[:, 1] > 1/40).astype(int)\n\n# Create a DataFrame for predicted labels using the same index as numero_de_cliente\npredicted_df = pd.DataFrame(y_pred_adjusted, columns=['Predicted'], index=dapply.index)\n\n# Combine the numero_de_cliente and predicted labels\nend_result = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result['Predicted'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#end_result['Predicted'].replace({1:0}, inplace=True)\nend_result['Predicted'].replace({2:1}, inplace=True)\nend_result['Predicted'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"end_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using 0.25 thresh\n\ny_pred_result = model.predict_proba(preprocessing.transform(dapply))\npredicted_df = pd.DataFrame((y_pred_result[:,1]>= 0.025).astype(int), columns=['Predicted'], index=dapply.index)\n# Combine the numero_de_cliente and predicted labels\nend_result = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result.to_csv(\"K101_001.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['KAGGLE_USERNAME'] = 'lechuck666'\nos.environ['KAGGLE_KEY'] = '3aaf0a00c8b35504b64d43d4592d6caf'\n!kaggle competitions submit -c laboratorio-de-imp-i-2023-virtual -f ./K101_001.csv -m \"2nd_less var\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}