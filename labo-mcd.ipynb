{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi\n\n!pip install featurewiz==0.2.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\narchivos = []\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        archivos.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mlflow\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\nimport category_encoders as ce\nimport os\n\n\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer, classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion    \nfrom sklearn.utils.validation import check_is_fitted\n\nfrom featurewiz import featurewiz\nfrom scipy import sparse\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 200)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:24:53.911252Z","iopub.execute_input":"2023-05-06T19:24:53.911717Z","iopub.status.idle":"2023-05-06T19:24:53.922355Z","shell.execute_reply.started":"2023-05-06T19:24:53.911670Z","shell.execute_reply":"2023-05-06T19:24:53.920972Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset_file = '/kaggle/input/lab1-complete-parquet/downsampled_df.csv'\n\ndataset = pd.read_csv(dataset_file)\ndataset.drop(columns='Unnamed: 0', inplace=True)\nprint(dataset_file)\ndataset['clase_ternaria'].replace({'BAJA+2': 1, 'BAJA+1':1, 'CONTINUA':0}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:24:55.473697Z","iopub.execute_input":"2023-05-06T19:24:55.474082Z","iopub.status.idle":"2023-05-06T19:24:56.735081Z","shell.execute_reply.started":"2023-05-06T19:24:55.474047Z","shell.execute_reply":"2023-05-06T19:24:56.733969Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/lab1-complete-parquet/downsampled_df.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def find_cols_with_high_nan(df, threshold=0.80, drop=True): \n    nan_pct = df.isna().sum() / len(df)    \n    # Select columns with NaN percentage greater than the threshold\n    high_nan_cols = nan_pct[nan_pct > threshold].index.tolist()\n    if drop:\n        print(f'dropeando {high_nan_cols} porque tienen mas de {threshold} Nan...')\n        return df.drop(columns=high_nan_cols)\n    return high_nan_cols\n\ndef join_visa_master(df):\n    print('juntando columnas con prefijo Visa_ y Master_')\n    visa_columns = df.filter(like='Visa_').columns.to_list()\n    master_columns = df.filter(like='Master_').columns.to_list()\n    \n    for col_visa, col_master in zip(visa_columns, master_columns):\n        new_column_name = 'sum_' + col_visa.split('_', 1)[1]\n        \n        # Replace NaN values with 0 temporarily for the sum\n        visa_values = df[col_visa].fillna(0)\n        master_values = df[col_master].fillna(0)\n        \n        # Sum the non-NaN values and store the result in the new column\n        df[new_column_name] = visa_values + master_values\n        df[new_column_name] = df[new_column_name].replace(0, np.nan)\n    \n    return df.drop(columns=visa_columns+master_columns)\n\n\ndef join_cols(df, string):\n    joined_cols = df.filter(like=string)\n    print(f'sumando las columnas: {joined_cols.columns.to_list()}...')\n    \n    # Replace NaN values with 0 temporarily for the sum\n    joined_cols_filled = joined_cols.fillna(0)\n    \n    # Calculate the sum\n    sum_values = joined_cols_filled.sum(axis=1)\n    \n    # Replace 0 values with NaN after the summation, but keep the original 0 values when both columns have 0 values in the same row\n    df[string] = sum_values.where((joined_cols_filled != 0).any(axis=1), joined_cols_filled.sum(axis=1))\n    \n    return df.drop(columns=joined_cols.columns)\n\ndef find_good_ratios(data):\n    '''crea ratios de todas las features float y se queda con los que featurewiz le da importancia, devuelve una lista de buenos candidatos para ratio'''\n    def _create_ratios(dataframe, variable,target):\n        float_cols = dataframe.select_dtypes(include='float64').columns.tolist()    \n        new_dataframe = pd.DataFrame()    \n        for col in float_cols:\n            if col != variable:\n                new_col_name = f'{variable}_to_{col}'\n                new_dataframe[new_col_name] = dataframe[variable] / dataframe[col]\n                new_dataframe.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n        new_dataframe[target] = dataframe[target]\n        return new_dataframe    \n    \n    the_best = []\n    for i in data.select_dtypes(include='float64').columns.tolist():\n        temp_df = _create_ratios(data, i,'clase_ternaria')\n        try:\n            best_ratios = featurewiz(temp_df, 'clase_ternaria',verbose=0)\n            the_best.append(best_ratios[0])\n        except:\n            the_best.append(f'error with {i}')\n    return the_best\n\ndef create_ratios_from_best(df, lista_best):\n    '''crea las features a partir de una lista de listas que tiene los mejores ratios'''\n    data = df.copy()    \n    for ratios in lista_best:\n        if \"error\" in ratios:\n            print(f\"Skipping invalid ratio: {ratios}\")\n            continue\n        else:\n            for j in ratios:\n                variables = j.split('_to_')\n                data[j] = data[variables[0]] / data[variables[1]]           \n    return data\n\ndef preprocessing_dataset(dataset, palabras_agrupar):\n    print(dataset.shape)\n    dataset = find_cols_with_high_nan(dataset)\n    print(dataset.shape)\n    dataset = join_visa_master(dataset)\n    print(dataset.shape)\n\n    for i in palabras_agrupar:    \n        dataset = join_cols(dataset, i)\n        print(dataset.shape)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:24:56.737588Z","iopub.execute_input":"2023-05-06T19:24:56.737977Z","iopub.status.idle":"2023-05-06T19:24:56.758465Z","shell.execute_reply.started":"2023-05-06T19:24:56.737937Z","shell.execute_reply":"2023-05-06T19:24:56.756897Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"palabras_agrupar =['prestamo','seguro','servicios', 'comisiones','cheques','ahorro','inversion','_consumo','descuentos','tarjeta','consumo','margen','debit','forex', 'transfer','autoservicio','cajas','atm','mobile']","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:24:56.760127Z","iopub.execute_input":"2023-05-06T19:24:56.760501Z","iopub.status.idle":"2023-05-06T19:24:56.766864Z","shell.execute_reply.started":"2023-05-06T19:24:56.760464Z","shell.execute_reply":"2023-05-06T19:24:56.765482Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset = preprocessing_dataset(dataset,palabras_agrupar)\n# list_of_best_ratios = find_good_ratios(dataset)\n# dataset = create_ratios_from_best(dataset, list_of_best_ratios)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:25:23.129509Z","iopub.execute_input":"2023-05-06T19:25:23.129894Z","iopub.status.idle":"2023-05-06T19:25:23.374657Z","shell.execute_reply.started":"2023-05-06T19:25:23.129859Z","shell.execute_reply":"2023-05-06T19:25:23.373435Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(86870, 67)\ndropeando ['sum_delinquency', 'sum_status', 'sum_msaldodolares', 'sum_madelantopesos', 'sum_madelantodolares', 'sum_mpagado', 'sum_mpagosdolares', 'sum_cadelantosefectivo'] porque tienen mas de 0.8 Nan...\n(86870, 59)\njuntando columnas con prefijo Visa_ y Master_\n(86870, 59)\nsumando las columnas: ['prestamo']...\n(86870, 58)\nsumando las columnas: ['seguro']...\n(86870, 57)\nsumando las columnas: ['servicios']...\n(86870, 56)\nsumando las columnas: ['comisiones']...\n(86870, 55)\nsumando las columnas: ['cheques']...\n(86870, 54)\nsumando las columnas: ['ahorro']...\n(86870, 53)\nsumando las columnas: ['inversion']...\n(86870, 52)\nsumando las columnas: []...\n(86870, 53)\nsumando las columnas: ['descuentos']...\n(86870, 52)\nsumando las columnas: ['tarjeta']...\n(86870, 51)\nsumando las columnas: ['consumo', '_consumo']...\n(86870, 49)\nsumando las columnas: ['margen']...\n(86870, 48)\nsumando las columnas: ['debit']...\n(86870, 47)\nsumando las columnas: ['forex']...\n(86870, 46)\nsumando las columnas: ['transfer']...\n(86870, 45)\nsumando las columnas: ['autoservicio']...\n(86870, 44)\nsumando las columnas: ['cajas']...\n(86870, 43)\nsumando las columnas: ['atm']...\n(86870, 42)\nsumando las columnas: ['mobile']...\n(86870, 41)\n","output_type":"stream"}]},{"cell_type":"code","source":"cols_keep_as_is = ['active_quarter','cliente_vip','internet','tcuentas','cdescubierto_preacordado','ccaja_seguridad','tcallcenter','thomebanking','cplazo_fijo','cajas','mobile','ccuenta_corriente','cpayroll2_trx','ctrx_quarter','cpayroll_trx']\ncols_to_binarize = ['cliente_edad','cliente_antiguedad','cproductos','cpagomiscuentas','ccallcenter_transacciones','chomebanking_transacciones','seguro']\ncols_to_normalize = [col for col in dataset.select_dtypes('float').columns if col not in cols_keep_as_is + cols_to_binarize]\n\n#len(cols_keep_as_is + cols_to_binarize+cols_to_normalize) == len(set(cols_keep_as_is + cols_to_binarize+cols_to_normalize))\nset(dataset.columns) - set(cols_keep_as_is + cols_to_binarize+cols_to_normalize)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:25:26.264135Z","iopub.execute_input":"2023-05-06T19:25:26.264543Z","iopub.status.idle":"2023-05-06T19:25:26.281752Z","shell.execute_reply.started":"2023-05-06T19:25:26.264506Z","shell.execute_reply":"2023-05-06T19:25:26.280557Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'clase_ternaria', 'foto_mes', 'numero_de_cliente'}"},"metadata":{}}]},{"cell_type":"code","source":"set(dataset.columns) - set(cols_keep_as_is + cols_to_binarize+cols_to_normalize)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:33:54.951220Z","iopub.execute_input":"2023-05-06T19:33:54.952260Z","iopub.status.idle":"2023-05-06T19:33:54.959564Z","shell.execute_reply.started":"2023-05-06T19:33:54.952206Z","shell.execute_reply":"2023-05-06T19:33:54.958408Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"set()"},"metadata":{}}]},{"cell_type":"code","source":"# simple type split\n\ncols_to_ignore = ['clase_ternaria', 'foto_mes', 'numero_de_cliente']\n\n\ncols_to_normalize = dataset.drop(columns=cols_to_ignore).select_dtypes('float').columns.to_list()\ncols_keep_as_is = dataset.drop(columns=cols_to_ignore).select_dtypes('int').columns.to_list()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:34:39.069445Z","iopub.execute_input":"2023-05-06T19:34:39.069847Z","iopub.status.idle":"2023-05-06T19:34:39.098804Z","shell.execute_reply.started":"2023-05-06T19:34:39.069810Z","shell.execute_reply":"2023-05-06T19:34:39.097829Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# pipeline preprocessing","metadata":{}},{"cell_type":"code","source":"class ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.columns]\n\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)\n    \n    \nclass PandasFeatureUnion(BaseEstimator, TransformerMixin):\n    def __init__(self, transformers):\n        self.transformers = transformers\n\n    def fit(self, X, y=None):\n        for _, transformer in self.transformers:\n            transformer.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        transformed_dfs = []\n        for _, transformer in self.transformers:\n            transformed_dfs.append(transformer.transform(X))        \n        \n        concatenated_df = pd.concat(transformed_dfs, axis=1)\n        return concatenated_df\n    \n\nclass BinTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins, columns=None):\n        self.n_bins = n_bins\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        self.quantiles_ = {}\n        self.outliers_ = {}\n        cols_to_transform = self.columns or X.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:          \n            q1, q3 = np.percentile(X[col], [25, 75])\n            iqr = q3 - q1\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n            mask = (X[col] < lower_bound) | (X[col] > upper_bound)\n            self.outliers_[col] = mask\n            col_no_outliers = X.loc[~mask, col]\n            self.quantiles_[col] = pd.qcut(col_no_outliers, self.n_bins, labels=False, duplicates='drop')\n        return self\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X_trans = X.copy()\n        cols_to_transform = self.columns or X_trans.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:         \n            X_trans[col] = pd.cut(X_trans[col], bins=self.n_bins, labels=False, duplicates='drop')\n            X_trans.loc[self.outliers_[col].reindex(X.index, fill_value=False).values, col] = 'outlier'\n            if col in self.quantiles_:\n                X_trans.loc[~self.outliers_[col].reindex(X.index, fill_value=False), col] = 'bin_' + self.quantiles_[col].apply(lambda x: str(x)).astype(str)      \n    \n        return X_trans[cols_to_transform]\n    \n    \nclass OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, categorical_cols=None):\n        self.categorical_cols = categorical_cols\n    \n    def fit(self, X, y=None):\n        if self.categorical_cols is None:\n            self.categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        self.categories_ = [X[col].astype('category').cat.categories.tolist() + ['dummy'] for col in self.categorical_cols]\n        self.encoder = OneHotEncoder(categories=self.categories_, handle_unknown='ignore')\n        self.encoder.fit(X[self.categorical_cols])\n        return self\n    \n    def transform(self, X, y=None):\n        X_transformed = self.encoder.transform(X[self.categorical_cols])\n        feature_names = self.encoder.get_feature_names_out(self.categorical_cols)\n        X_transformed_df = pd.DataFrame.sparse.from_spmatrix(X_transformed, columns=feature_names)\n        X_transformed_df.index = X.index\n        return pd.concat([X.drop(columns=self.categorical_cols), X_transformed_df], axis=1)\n\n\nclass TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, cols=None, drop_invariant=False, return_df=True):\n        self.cols = cols\n        self.drop_invariant = drop_invariant\n        self.return_df = return_df\n        self.encoder = ce.TargetEncoder(cols=self.cols, drop_invariant=self.drop_invariant, return_df=self.return_df)\n\n    def fit(self, X, y=None):\n        self.encoder.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        return self.encoder.transform(X, y)    \n    \n\nclass SimpleImputerWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=strategy)\n    \n    def fit(self, X, y=None):\n        self.imputer.fit(X)\n        return self\n    \n    def transform(self, X):\n        X_transformed = self.imputer.transform(X)\n        return pd.DataFrame(X_transformed, columns=X.columns, index=X.index)\n    \n    \nclass PercentageVariationTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, variable):\n        self.variable = variable\n\n    def fit(self, X, y):\n        # Combine X and y for easier calculations\n        data = pd.concat([X, y], axis=1)\n\n        # Calculate the mean of the variable for different target values\n        self.target_means = data.groupby(y.name)[self.variable].mean()\n        return self\n\n    def transform(self, X, y=None):\n        result = pd.DataFrame()\n\n        # Calculate the percentage variation for each target mean\n        for target_value, target_mean in self.target_means.items():\n            column_name = f\"{self.variable}_pct_variation_{target_value}\"\n            result[column_name] = (X[self.variable] - target_mean) / target_mean * 100\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:34:41.481139Z","iopub.execute_input":"2023-05-06T19:34:41.481502Z","iopub.status.idle":"2023-05-06T19:34:41.515202Z","shell.execute_reply.started":"2023-05-06T19:34:41.481469Z","shell.execute_reply":"2023-05-06T19:34:41.514020Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## feature engineering","metadata":{}},{"cell_type":"code","source":"class RatioFeature(BaseEstimator, TransformerMixin):\n    def __init__(self, base_var, other_vars):\n        self.base_var = base_var\n        self.other_vars = other_vars if other_vars is not None else []\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Verificar que base_var existe en el DataFrame\n        if self.base_var not in X.columns:\n            raise ValueError(f\"La columna '{self.base_var}' no se encuentra en el DataFrame\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas features\n        new_features = pd.DataFrame()\n\n        # Calcular las nuevas features\n        for var in self.other_vars:\n            if var in X.columns:\n                feature_name = f\"{self.base_var}_{var}_ratio\"\n                new_features[feature_name] = X[self.base_var] / X[var]\n            else:\n                raise ValueError(f\"La columna '{var}' no se encuentra en el DataFrame\")\n        return new_features\n    \n    \nclass JoinColumnsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, like_strings):\n        self.like_strings = like_strings\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas columnas\n        new_cols = pd.DataFrame(index=X.index)\n\n        # Iterar sobre las cadenas especificadas\n        for like_string in self.like_strings:\n            # Filtrar las columnas que contienen la cadena específica\n            joined_cols = X.filter(like=like_string)\n\n            # Crear una nueva columna con la suma de las columnas filtradas\n            new_col_name = 'sum_'+like_string\n            new_cols[new_col_name] = joined_cols.sum(axis=1)\n        return new_cols\n    \n\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)    \n    \n\n\nclass ReplaceInfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, factor = 3):\n        self.column_max_values = {}\n        self.column_min_values = {}\n        self.factor = factor\n\n    def fit(self, X, y=None):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input should be a pandas DataFrame\")\n\n        for column in X.columns:\n            max_value = X[column].replace([np.inf, -np.inf], np.nan).max()\n            min_value = X[column].replace([np.inf, -np.inf], np.nan).min()\n\n            self.column_max_values[column] = max_value\n            self.column_min_values[column] = min_value\n\n        return self\n\n    def transform(self, X, y=None):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input should be a pandas DataFrame\")\n\n        X_copy = X.copy()\n\n        for column in X_copy.columns:\n            if column in self.column_max_values:\n                X_copy[column] = X_copy[column].replace(np.inf, self.column_max_values[column] * self.factor)\n            if column in self.column_min_values:\n                X_copy[column] = X_copy[column].replace(-np.inf, self.column_min_values[column] * self.factor)\n\n        return X_copy","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:34:42.091965Z","iopub.execute_input":"2023-05-06T19:34:42.092375Z","iopub.status.idle":"2023-05-06T19:34:42.114101Z","shell.execute_reply.started":"2023-05-06T19:34:42.092340Z","shell.execute_reply":"2023-05-06T19:34:42.112495Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# binning = Pipeline([\n#     ('col_select',ColumnSelector(columns=cols_to_binarize)),\n#     ('bin_convert',BinTransformer(columns=None, n_bins = 3)),\n#     ('onehot', OneHotEncoderTransformer())\n#     #('target_encoding', TargetEncoderWrapper()) \n# ])\n\nstandarize = Pipeline([\n    ('col_select',ColumnSelector(columns=cols_to_normalize)),\n    ('scale', StandardScalerTransformer(columns=cols_to_normalize)),\n])\n\nfeature_engineering = PandasFeatureUnion([\n    ('as_is', ColumnSelector(columns=cols_keep_as_is)),\n    #('log', standarize)   \n])\n\npreprocessing = Pipeline([    \n    ('inf', ReplaceInfTransformer()),\n    #('fillna', SimpleImputerWrapper(strategy='mean')),\n    ('feature_engineering', feature_engineering)\n])","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:35:20.230750Z","iopub.execute_input":"2023-05-06T19:35:20.231152Z","iopub.status.idle":"2023-05-06T19:35:20.238906Z","shell.execute_reply.started":"2023-05-06T19:35:20.231115Z","shell.execute_reply":"2023-05-06T19:35:20.236838Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"X = dataset.drop('clase_ternaria', axis=1)\ny = dataset['clase_ternaria']","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:35:21.178673Z","iopub.execute_input":"2023-05-06T19:35:21.179139Z","iopub.status.idle":"2023-05-06T19:35:21.191487Z","shell.execute_reply.started":"2023-05-06T19:35:21.179090Z","shell.execute_reply":"2023-05-06T19:35:21.190075Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n\n# Fit the preprocessing pipeline on the training set and transform both the training and test sets\nX_train_processed = preprocessing.fit_transform(X_train, y_train)\nX_test_processed = preprocessing.transform(X_test)\n\n# Check the shapes of the processed data\nprint(X_train_processed.shape, X_test_processed.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:35:21.762270Z","iopub.execute_input":"2023-05-06T19:35:21.763226Z","iopub.status.idle":"2023-05-06T19:35:21.902455Z","shell.execute_reply.started":"2023-05-06T19:35:21.763171Z","shell.execute_reply":"2023-05-06T19:35:21.901262Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"(60809, 19) (26061, 19)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## model evaluation","metadata":{}},{"cell_type":"code","source":"def get_balanced_classes():\n    class_counts = y.value_counts()\n    total_samples = class_counts.sum()\n    class_frequencies = class_counts / total_samples\n    class_weights = 1 / class_frequencies\n    return class_weights.to_dict()\n\nclass_weights_dict = get_balanced_classes()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:35:41.502380Z","iopub.execute_input":"2023-05-06T19:35:41.502758Z","iopub.status.idle":"2023-05-06T19:35:41.511307Z","shell.execute_reply.started":"2023-05-06T19:35:41.502724Z","shell.execute_reply":"2023-05-06T19:35:41.509879Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import mlflow","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:36:20.718579Z","iopub.execute_input":"2023-05-06T19:36:20.718948Z","iopub.status.idle":"2023-05-06T19:36:21.105747Z","shell.execute_reply.started":"2023-05-06T19:36:20.718916Z","shell.execute_reply":"2023-05-06T19:36:21.104673Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"mlflow.set_experiment(\"first_baseline\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:36:33.282504Z","iopub.execute_input":"2023-05-06T19:36:33.283678Z","iopub.status.idle":"2023-05-06T19:36:33.296922Z","shell.execute_reply.started":"2023-05-06T19:36:33.283630Z","shell.execute_reply":"2023-05-06T19:36:33.295600Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"2023/05/06 19:36:33 INFO mlflow.tracking.fluent: Experiment with name 'first_baseline' does not exist. Creating a new experiment.\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='file:///kaggle/working/mlruns/1', creation_time=1683401793291, experiment_id='1', last_update_time=1683401793291, lifecycle_stage='active', name='first_baseline', tags={}>"},"metadata":{}}]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import early_stopping, log_evaluation\n\ndef objective(trial):\n    \n    kf =  KFold(n_splits=3, shuffle=True)    \n    f1_scores = []\n    \n    for train_index, val_index in kf.split(X_train_processed):\n        X_tr, X_val = X_train_processed.iloc[train_index], X_train_processed.iloc[val_index]\n        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        params = {\n#             \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1.0, log=True),\n#             \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n#             \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 128),  # Fix the maximum value for num_leaves\n            \n#             'class_weight': class_weights_dict,  \n#             \"objective\": \"multiclass\",\n#             \"metric\": \"multi_logloss\",\n#             \"num_class\": 3,\n#             \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n#             \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n#             \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n#             \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 1e-8, 1.0)\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.3),\n            \"max_depth\": -1,\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 1024), \n            \"verbosity\": -1,\n            \"boosting_type\": \"gbdt\",   \n            'n_jobs': -1,\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),        \n            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100, 50000),\n            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.01, 1.0),\n            \"extra_trees\": True,\n            \"seed\": 42,  # Replace 42 with the desired seed value\n            \n#             \"boosting_type\": \"gbdt\",\n#             \"objective\": \"binary\",\n#             \"metric\": \"custom\",\n#             \"verbosity\": -100,\n#             'n_jobs': -1,\n#             \"boost_from_average\": True,\n#             \"feature_pre_filter\": False,\n#             \"force_row_wise\": True,\n#             \"min_gain_to_split\": 0.0,\n#             \"min_sum_hessian_in_leaf\": 0.001,\n#             \"lambda_l1\": 0.0,\n#             \"lambda_l2\": 0.0,\n#             \"max_bin\": 31,\n#             \"num_iterations\": 9999,\n#             \"bagging_fraction\": 1.0,\n#             \"pos_bagging_fraction\": 1.0,\n#             \"neg_bagging_fraction\": 1.0,\n#             \"is_unbalance\": False,\n#             \"scale_pos_weight\": 1.0,\n#             \"drop_rate\": 0.1,\n#             \"max_drop\": 50,\n#             \"skip_drop\": 0.5            \n\n        }\n\n        clf = lgb.LGBMClassifier(device='gpu', **params)\n        \n        try:\n            clf.fit(\n                X_tr, y_tr, \n                eval_set=[(X_val, y_val)], \n                callbacks=[early_stopping(50), lgb.log_evaluation(period=50)]\n            )    \n        \n        except lgb.basic.LightGBMError as e:\n            print(f\"Trial {trial.number} failed with parameters: {params}\")\n            if len(trial.study.trials) > 1:\n                print(f\"Current best parameters: {trial.study.best_params}\")\n            return -float('inf')  # Return very low score to skip this set of hyperparameters\n\n            \n        y_pred = clf.predict(X_val)\n        f1 = f1_score(y_val, y_pred, average='macro')\n        f1_scores.append(f1)\n\n    return sum(f1_scores) / len(f1_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nimport mlflow\nimport mlflow.lightgbm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import early_stopping, log_evaluation\n\n# Configurar el experimento en MLflow\nmlflow.set_experiment(\"Optuna_LightGBM_Kaggle\")\n\ndef objective(trial):\n    kf =  KFold(n_splits=3, shuffle=True)\n\n    f1_scores = []\n    for train_index, val_index in kf.split(X_train_processed):\n        X_tr, X_val = X_train_processed.iloc[train_index], X_train_processed.iloc[val_index]\n        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        params = {\n            \"verbosity\": -1,\n            \"boosting_type\": \"gbdt\",\n            'n_jobs': -1,\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000), \n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.3),\n            \"max_depth\": -1,\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 1024), \n            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100, 50000),\n            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.01, 1.0),\n            \"extra_trees\": True,\n            \"seed\": 42\n        }\n\n        with mlflow.start_run():\n            # Registra los parámetros del modelo\n            for param_name, param_value in params.items():\n                mlflow.log_param(param_name, param_value)\n            \n            clf = lgb.LGBMClassifier(device='gpu', **params)\n\n            try:\n                clf.fit(\n                    X_tr, y_tr,\n                    eval_set=[(X_val, y_val)],\n                    callbacks=[early_stopping(50), lgb.log_evaluation(period=50)]\n                )\n\n                y_pred = clf.predict(X_val)\n                f1 = f1_score(y_val, y_pred, average='macro')\n                f1_scores.append(f1)\n\n                # Registra la métrica F1\n                mlflow.log_metric(\"f1_score\", f1)\n\n                # Registra el modelo LightGBM\n                mlflow.lightgbm.log_model(clf, \"lightgbm_model\")\n\n            except lgb.basic.LightGBMError as e:\n                print(f\"Trial {trial.number} failed with parameters: {params}\")\n                if len(trial.study.trials) > 1:\n                    print(f\"Current best parameters: {trial.study.best_params}\")\n                return -float('inf')\n\n    return sum(f1_scores) / len(f1_scores)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:40:19.409311Z","iopub.execute_input":"2023-05-06T19:40:19.409943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize_lgbm_hyperparameters():\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(f\"Best trial: {study.best_trial.params}\")\n    print(f\"Best F1 score: {study.best_value}\")\n    return study.best_trial.params\n\nbest_params = optimize_lgbm_hyperparameters()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:40:19.409311Z","iopub.execute_input":"2023-05-06T19:40:19.409943Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2023/05/06 19:40:19 INFO mlflow.tracking.fluent: Experiment with name 'Optuna_LightGBM_Kaggle' does not exist. Creating a new experiment.\n\u001b[32m[I 2023-05-06 19:40:19,429]\u001b[0m A new study created in memory with name: no-name-32153ed2-0a92-4b57-ba7b-ee64f9bf6431\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=10449, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10449\n[LightGBM] [Warning] feature_fraction is set=0.2986554182543892, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2986554182543892\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.484294\n[100]\tvalid_0's binary_logloss: 0.454898\n[150]\tvalid_0's binary_logloss: 0.448335\n[200]\tvalid_0's binary_logloss: 0.4462\n[250]\tvalid_0's binary_logloss: 0.445338\n[300]\tvalid_0's binary_logloss: 0.444075\n[350]\tvalid_0's binary_logloss: 0.443247\n[400]\tvalid_0's binary_logloss: 0.44266\n[450]\tvalid_0's binary_logloss: 0.442417\n[500]\tvalid_0's binary_logloss: 0.442185\n[550]\tvalid_0's binary_logloss: 0.441893\n[600]\tvalid_0's binary_logloss: 0.441573\n[650]\tvalid_0's binary_logloss: 0.441144\n[700]\tvalid_0's binary_logloss: 0.440779\n[750]\tvalid_0's binary_logloss: 0.44052\nEarly stopping, best iteration is:\n[743]\tvalid_0's binary_logloss: 0.440517\n","output_type":"stream"},{"name":"stderr","text":"2023/05/06 19:40:32 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpq_i52880/model, flavor: lightgbm), fall back to return ['lightgbm==3.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=10449, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10449\n[LightGBM] [Warning] feature_fraction is set=0.2986554182543892, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2986554182543892\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.491644\n[100]\tvalid_0's binary_logloss: 0.460658\n[150]\tvalid_0's binary_logloss: 0.452501\n[200]\tvalid_0's binary_logloss: 0.45046\n[250]\tvalid_0's binary_logloss: 0.450161\n[300]\tvalid_0's binary_logloss: 0.449126\n[350]\tvalid_0's binary_logloss: 0.448566\n[400]\tvalid_0's binary_logloss: 0.447837\n[450]\tvalid_0's binary_logloss: 0.447722\n[500]\tvalid_0's binary_logloss: 0.447626\n[550]\tvalid_0's binary_logloss: 0.447358\n[600]\tvalid_0's binary_logloss: 0.447089\n[650]\tvalid_0's binary_logloss: 0.446846\n[700]\tvalid_0's binary_logloss: 0.446773\n[750]\tvalid_0's binary_logloss: 0.446631\n[800]\tvalid_0's binary_logloss: 0.446494\n[850]\tvalid_0's binary_logloss: 0.446453\n[900]\tvalid_0's binary_logloss: 0.446442\nDid not meet early stopping. Best iteration is:\n[943]\tvalid_0's binary_logloss: 0.446385\n","output_type":"stream"},{"name":"stderr","text":"2023/05/06 19:40:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpb9o1_kfd/model, flavor: lightgbm), fall back to return ['lightgbm==3.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=10449, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10449\n[LightGBM] [Warning] feature_fraction is set=0.2986554182543892, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2986554182543892\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.472158\n[100]\tvalid_0's binary_logloss: 0.450901\n[150]\tvalid_0's binary_logloss: 0.446964\n[200]\tvalid_0's binary_logloss: 0.444918\n[250]\tvalid_0's binary_logloss: 0.443446\n[300]\tvalid_0's binary_logloss: 0.442752\n[350]\tvalid_0's binary_logloss: 0.442443\n[400]\tvalid_0's binary_logloss: 0.442186\n[450]\tvalid_0's binary_logloss: 0.442125\n[500]\tvalid_0's binary_logloss: 0.441981\n[550]\tvalid_0's binary_logloss: 0.441953\n[600]\tvalid_0's binary_logloss: 0.44167\n[650]\tvalid_0's binary_logloss: 0.441523\n[700]\tvalid_0's binary_logloss: 0.441498\n[750]\tvalid_0's binary_logloss: 0.441433\n[800]\tvalid_0's binary_logloss: 0.44126\n[850]\tvalid_0's binary_logloss: 0.44123\n[900]\tvalid_0's binary_logloss: 0.441141\nDid not meet early stopping. Best iteration is:\n[939]\tvalid_0's binary_logloss: 0.441121\n","output_type":"stream"},{"name":"stderr","text":"2023/05/06 19:40:45 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp5cdxt201/model, flavor: lightgbm), fall back to return ['lightgbm==3.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n\u001b[32m[I 2023-05-06 19:40:45,891]\u001b[0m Trial 0 finished with value: 0.804196514882748 and parameters: {'n_estimators': 944, 'learning_rate': 0.16025209055242778, 'num_leaves': 919, 'min_data_in_leaf': 10449, 'feature_fraction': 0.2986554182543892}. Best is trial 0 with value: 0.804196514882748.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=30810, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30810\n[LightGBM] [Warning] feature_fraction is set=0.03631977414214829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.03631977414214829\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.693138\nEarly stopping, best iteration is:\n[1]\tvalid_0's binary_logloss: 0.693138\n[LightGBM] [Warning] min_data_in_leaf is set=30810, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30810\n[LightGBM] [Warning] feature_fraction is set=0.03631977414214829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.03631977414214829\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.693185\nEarly stopping, best iteration is:\n[1]\tvalid_0's binary_logloss: 0.693185\n[LightGBM] [Warning] min_data_in_leaf is set=30810, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30810\n[LightGBM] [Warning] feature_fraction is set=0.03631977414214829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.03631977414214829\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.693143\nEarly stopping, best iteration is:\n[1]\tvalid_0's binary_logloss: 0.693143\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2023-05-06 19:41:02,681]\u001b[0m Trial 1 finished with value: 0.33389159212908 and parameters: {'n_estimators': 648, 'learning_rate': 0.25203014807266366, 'num_leaves': 1011, 'min_data_in_leaf': 30810, 'feature_fraction': 0.03631977414214829}. Best is trial 0 with value: 0.804196514882748.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=11469, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11469\n[LightGBM] [Warning] feature_fraction is set=0.48218635106797, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48218635106797\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.538707\n[100]\tvalid_0's binary_logloss: 0.501199\n[150]\tvalid_0's binary_logloss: 0.486722\n[200]\tvalid_0's binary_logloss: 0.478583\n[250]\tvalid_0's binary_logloss: 0.477662\n[300]\tvalid_0's binary_logloss: 0.472794\n[350]\tvalid_0's binary_logloss: 0.471483\n[400]\tvalid_0's binary_logloss: 0.470311\n[450]\tvalid_0's binary_logloss: 0.469736\n[500]\tvalid_0's binary_logloss: 0.469069\n[550]\tvalid_0's binary_logloss: 0.468943\n[600]\tvalid_0's binary_logloss: 0.468723\n[650]\tvalid_0's binary_logloss: 0.468611\n[700]\tvalid_0's binary_logloss: 0.468526\n[750]\tvalid_0's binary_logloss: 0.468409\n[800]\tvalid_0's binary_logloss: 0.468238\nDid not meet early stopping. Best iteration is:\n[831]\tvalid_0's binary_logloss: 0.467973\n[LightGBM] [Warning] min_data_in_leaf is set=11469, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11469\n[LightGBM] [Warning] feature_fraction is set=0.48218635106797, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48218635106797\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.526416\n[100]\tvalid_0's binary_logloss: 0.500165\n[150]\tvalid_0's binary_logloss: 0.483322\n[200]\tvalid_0's binary_logloss: 0.473141\n[250]\tvalid_0's binary_logloss: 0.471973\n[300]\tvalid_0's binary_logloss: 0.466133\n[350]\tvalid_0's binary_logloss: 0.464487\n[400]\tvalid_0's binary_logloss: 0.463068\n[450]\tvalid_0's binary_logloss: 0.462333\n[500]\tvalid_0's binary_logloss: 0.461995\n[550]\tvalid_0's binary_logloss: 0.461901\n[600]\tvalid_0's binary_logloss: 0.461694\n[650]\tvalid_0's binary_logloss: 0.461585\n[700]\tvalid_0's binary_logloss: 0.461381\n[750]\tvalid_0's binary_logloss: 0.461281\n[800]\tvalid_0's binary_logloss: 0.460998\nDid not meet early stopping. Best iteration is:\n[830]\tvalid_0's binary_logloss: 0.460752\n[LightGBM] [Warning] min_data_in_leaf is set=11469, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11469\n[LightGBM] [Warning] feature_fraction is set=0.48218635106797, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48218635106797\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.530687\n[100]\tvalid_0's binary_logloss: 0.483762\n[150]\tvalid_0's binary_logloss: 0.472907\n[200]\tvalid_0's binary_logloss: 0.467835\n[250]\tvalid_0's binary_logloss: 0.46311\n[300]\tvalid_0's binary_logloss: 0.460282\n[350]\tvalid_0's binary_logloss: 0.458815\n[400]\tvalid_0's binary_logloss: 0.457443\n[450]\tvalid_0's binary_logloss: 0.456738\n[500]\tvalid_0's binary_logloss: 0.456336\n[550]\tvalid_0's binary_logloss: 0.456068\n[600]\tvalid_0's binary_logloss: 0.455957\n[650]\tvalid_0's binary_logloss: 0.455797\n[700]\tvalid_0's binary_logloss: 0.45575\n[750]\tvalid_0's binary_logloss: 0.4557\n[800]\tvalid_0's binary_logloss: 0.455622\nDid not meet early stopping. Best iteration is:\n[813]\tvalid_0's binary_logloss: 0.45559\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2023-05-06 19:41:22,346]\u001b[0m Trial 2 finished with value: 0.7958439392683351 and parameters: {'n_estimators': 832, 'learning_rate': 0.08779099158601494, 'num_leaves': 365, 'min_data_in_leaf': 11469, 'feature_fraction': 0.48218635106797}. Best is trial 0 with value: 0.804196514882748.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=34339, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34339\n[LightGBM] [Warning] feature_fraction is set=0.20929673124770984, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20929673124770984\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.693167\nEarly stopping, best iteration is:\n[1]\tvalid_0's binary_logloss: 0.693167\n[LightGBM] [Warning] min_data_in_leaf is set=34339, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34339\n[LightGBM] [Warning] feature_fraction is set=0.20929673124770984, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20929673124770984\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.693141\nEarly stopping, best iteration is:\n[1]\tvalid_0's binary_logloss: 0.693141\n[LightGBM] [Warning] min_data_in_leaf is set=34339, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34339\n[LightGBM] [Warning] feature_fraction is set=0.20929673124770984, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20929673124770984\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.693169\nEarly stopping, best iteration is:\n[1]\tvalid_0's binary_logloss: 0.693169\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2023-05-06 19:41:35,093]\u001b[0m Trial 3 finished with value: 0.33389139517739097 and parameters: {'n_estimators': 972, 'learning_rate': 0.21682736037171904, 'num_leaves': 889, 'min_data_in_leaf': 34339, 'feature_fraction': 0.20929673124770984}. Best is trial 0 with value: 0.804196514882748.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=18342, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18342\n[LightGBM] [Warning] feature_fraction is set=0.5847561406179358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5847561406179358\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.620124\n[100]\tvalid_0's binary_logloss: 0.579883\n[150]\tvalid_0's binary_logloss: 0.56197\n[200]\tvalid_0's binary_logloss: 0.548941\n[250]\tvalid_0's binary_logloss: 0.540275\n[300]\tvalid_0's binary_logloss: 0.540188\n[350]\tvalid_0's binary_logloss: 0.5308\n[400]\tvalid_0's binary_logloss: 0.523748\nEarly stopping, best iteration is:\n[373]\tvalid_0's binary_logloss: 0.523739\n[LightGBM] [Warning] min_data_in_leaf is set=18342, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18342\n[LightGBM] [Warning] feature_fraction is set=0.5847561406179358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5847561406179358\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's binary_logloss: 0.626082\n[100]\tvalid_0's binary_logloss: 0.58355\n[150]\tvalid_0's binary_logloss: 0.564703\n[200]\tvalid_0's binary_logloss: 0.551377\n[250]\tvalid_0's binary_logloss: 0.542729\n[300]\tvalid_0's binary_logloss: 0.542541\n[350]\tvalid_0's binary_logloss: 0.53783\n[400]\tvalid_0's binary_logloss: 0.530088\n[450]\tvalid_0's binary_logloss: 0.527138\n[500]\tvalid_0's binary_logloss: 0.515917\n[550]\tvalid_0's binary_logloss: 0.514525\n[600]\tvalid_0's binary_logloss: 0.512077\nDid not meet early stopping. Best iteration is:\n[601]\tvalid_0's binary_logloss: 0.511603\n","output_type":"stream"}]},{"cell_type":"code","source":"mlflow ui","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:42:33.280559Z","iopub.execute_input":"2023-05-06T19:42:33.281064Z","iopub.status.idle":"2023-05-06T19:42:33.290642Z","shell.execute_reply.started":"2023-05-06T19:42:33.281017Z","shell.execute_reply":"2023-05-06T19:42:33.288690Z"},"trusted":true},"execution_count":64,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_23/3574192917.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mlflow ui\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (3574192917.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"best_params","metadata":{"execution":{"iopub.status.busy":"2023-05-06T19:42:30.351426Z","iopub.status.idle":"2023-05-06T19:42:30.351787Z","shell.execute_reply.started":"2023-05-06T19:42:30.351611Z","shell.execute_reply":"2023-05-06T19:42:30.351630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nfrom lightgbm import LGBMClassifier\n\n#class_weight = {0:1, 1:40}\n\nmodel = LGBMClassifier(device='gpu', **best_params)\n\nmodel.fit(X_train_processed, y_train)\n\ny_pred_proba = model.predict_proba(X_test_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test_processed)\n\ncm = confusion_matrix(y_test, y_pred)\nax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nplt.show()\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prediccion","metadata":{}},{"cell_type":"code","source":"# Read the dataset\npredict_set = pd.read_csv('/kaggle/input/dset-peq/dataset_pequeno.csv')\n\ndapply = predict_set[predict_set['foto_mes'] == 202109].drop('clase_ternaria', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dapply = preprocessing_dataset(dapply,palabras_agrupar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dapply = create_ratios_from_best(dapply, list_of_best_ratios)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_result = model.predict_proba(preprocessing.transform(dapply))\n\n# Create a DataFrame for predicted labels using the same index as numero_de_cliente\npredicted_df = pd.DataFrame(y_pred_result[:, 1], columns=['proba'], index=dapply.index)\n\n# Combine the numero_de_cliente and predicted labels\nend_res = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nth(df_end, n):\n    end_res = df_end.copy()\n    get_nth = end_res.sort_values('proba', ascending=False).head(n).iloc[-1]['proba']\n    end_res['Predicted'] = (end_res['proba'] >= get_nth).astype(int)\n    end_res.drop(columns='proba', inplace=True)\n    return end_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 9000\n\nend_result = get_nth(end_res,n=n)\nend_result['Predicted'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result.to_csv(\"K101_001.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['KAGGLE_USERNAME'] = 'lechuck666'\nos.environ['KAGGLE_KEY'] = '3aaf0a00c8b35504b64d43d4592d6caf'\n!kaggle competitions submit -c laboratorio-de-imp-i-2023-virtual -f ./K101_001.csv -m \"2nd_less var\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}