{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-10T01:10:51.682218Z","iopub.execute_input":"2023-04-10T01:10:51.683286Z","iopub.status.idle":"2023-04-10T01:10:51.704256Z","shell.execute_reply.started":"2023-04-10T01:10:51.683236Z","shell.execute_reply":"2023-04-10T01:10:51.702881Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dset-pequenio/DiccionarioDatos_2023.ods\n/kaggle/input/dset-peq/dataset_pequeno.csv\n/kaggle/input/laboratorio-de-imp-i-2023-virtual/kaggle_competencia_muestra.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\nimport os\n\n\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom featurewiz import featurewiz\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom scipy import sparse\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 200)\n\n# Read the dataset\ndataset = pd.read_csv('/kaggle/input/dset-peq/dataset_pequeno.csv')\n\ndtrain = dataset[dataset['foto_mes'] == 202107].copy()\ndapply = dataset[dataset['foto_mes'] == 202109].drop('clase_ternaria', axis=1)\n\ndtrain['clase_ternaria'].replace({'BAJA+2': 2, 'BAJA+1':1, 'CONTINUA':0}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:10:51.705578Z","iopub.execute_input":"2023-04-10T01:10:51.706259Z","iopub.status.idle":"2023-04-10T01:10:58.801807Z","shell.execute_reply.started":"2023-04-10T01:10:51.706219Z","shell.execute_reply":"2023-04-10T01:10:58.800413Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stdout","text":"Imported 0.2.6 version. Select nrows to a small number when running on huge datasets.\noutput = featurewiz(dataname, target, corr_limit=0.90, verbose=2, sep=',', \n\t\theader=0, test_data='',feature_engg='', category_encoders='',\n\t\tdask_xgboost_flag=False, nrows=None, skip_sulov=False)\nCreate new features via 'feature_engg' flag : ['interactions','groupby','target']\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"dapply.shape, dtrain.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:10:58.807131Z","iopub.execute_input":"2023-04-10T01:10:58.809623Z","iopub.status.idle":"2023-04-10T01:10:58.820992Z","shell.execute_reply.started":"2023-04-10T01:10:58.809579Z","shell.execute_reply":"2023-04-10T01:10:58.819683Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((165237, 154), (164682, 155))"},"metadata":{}}]},{"cell_type":"code","source":"def preproc_func(dtrain, agrup):\n    '''elimina nulos, joinea columnas con conceptos similares'''\n\n    def find_cols_with_high_nan(df, threshold=0.80): \n        nan_pct = df.isna().sum() / len(df)    \n        # Select columns with NaN percentage greater than the threshold\n        high_nan_cols = nan_pct[nan_pct > threshold].index.tolist()    \n        return high_nan_cols\n\n    def join_cols(df, string):\n        joined_cols = df.filter(like=string)\n        df[string] = joined_cols.sum(axis=1)\n        return dtrain.drop(columns=joined_cols.columns)\n\n    # Get the Visa and Master columns lists\n    visa_columns = dtrain.filter(like='Visa_').columns.to_list()\n    master_columns = dtrain.filter(like='Master_').columns.to_list()\n\n    # Iterate through the corresponding columns and create the new columns in the DataFrame\n    for col_visa, col_master in zip(visa_columns, master_columns):\n        new_column_name = 'sum_' + col_visa.split('_', 1)[1]  # Remove the 'Visa_' prefix and add 'sum_' prefix\n        dtrain[new_column_name] = dtrain[col_visa] + dtrain[col_master]\n\n    dtrain.drop(columns=visa_columns+master_columns, inplace=True)\n\n    for i in agrup:\n        dtrain = join_cols(dtrain, i)\n\n    cols_to_drop =['ccuenta_corriente','foto_mes'] + find_cols_with_high_nan(dtrain)\n    print(f'dropping {cols_to_drop}')\n    dtrain = dtrain.drop(columns=cols_to_drop)    \n    return dtrain\n\n\ndef find_good_ratios(data):\n    '''crea ratios de todas las features float y se queda con los que featurewiz le da importancia, devuelve una lista de buenos candidatos para ratio'''\n    def _create_ratios(dataframe, variable,target):\n        float_cols = dataframe.select_dtypes(include='float64').columns.tolist()    \n        new_dataframe = pd.DataFrame()    \n        for col in float_cols:\n            if col != variable:\n                new_col_name = f'{variable}_to_{col}'\n                new_dataframe[new_col_name] = dataframe[variable] / dataframe[col]\n\n        new_dataframe[target] = dataframe[target]\n        return new_dataframe    \n    \n    the_best = []\n    for i in data.select_dtypes(include='float64').columns.tolist():\n        temp_df = _create_ratios(data, i,'clase_ternaria')\n        try:\n            best_ratios = featurewiz(temp_df, 'clase_ternaria')\n            the_best.append(best_ratios[0])\n        except:\n            the_best.append(f'error with {i}')\n    return the_best\n\n\ndef create_ratios_from_best(df, lista_best):\n    '''crea las features a partir de una lista de listas que tiene los mejores ratios'''\n    data = df.copy()    \n    for ratios in lista_best:\n        if \"error\" in ratios:\n            print(f\"Skipping invalid ratio: {ratios}\")\n            continue\n        else:\n            for j in ratios:\n                variables = j.split('_to_')\n                data[j] = data[variables[0]] / data[variables[1]]           \n    return data\n\n\ndef undersample_multiclass(dataframe, target_col):\n    X = dataframe.drop(target_col, axis=1)\n    y = dataframe[target_col]\n    rus = RandomUnderSampler()   \n    X_resampled, y_resampled = rus.fit_resample(X, y)\n    resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n    return resampled_df","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:10:58.827046Z","iopub.execute_input":"2023-04-10T01:10:58.829450Z","iopub.status.idle":"2023-04-10T01:10:58.852808Z","shell.execute_reply.started":"2023-04-10T01:10:58.829409Z","shell.execute_reply":"2023-04-10T01:10:58.851369Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"palabras_agrupar =['prestamo','seguro','servicios', 'comisiones','cheques','ahorro','inversion','tarjeta','_consumo','margen','debit','forex', 'transfer','autoservicio','cajas','atm','trx','mobile']\n\ndtrain = preproc_func(dtrain,palabras_agrupar)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:10:59.867503Z","iopub.execute_input":"2023-04-10T01:10:59.868203Z","iopub.status.idle":"2023-04-10T01:11:01.568058Z","shell.execute_reply.started":"2023-04-10T01:10:59.868163Z","shell.execute_reply":"2023-04-10T01:11:01.566869Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"dropping ['ccuenta_corriente', 'foto_mes', 'sum_Finiciomora']\n","output_type":"stream"}]},{"cell_type":"code","source":"#undersamplear para ver importancia \nresampled_df = undersample_multiclass(dtrain, 'clase_ternaria')\nbest_unb = find_good_ratios(resampled_df)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:11:08.311289Z","iopub.execute_input":"2023-04-10T01:11:08.311919Z","iopub.status.idle":"2023-04-10T01:12:16.510639Z","shell.execute_reply.started":"2023-04-10T01:11:08.311878Z","shell.execute_reply":"2023-04-10T01:12:16.509818Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 63372 rows and 38 columns with infinity in them...\n        38 variable(s) to be removed since ID or low-information variables\n    \tmore than 38 variables to be removed; too many to print...\ntrain data shape before dropping 38 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 8)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 7 features ############\n#######################################################################################\nSelecting all (7) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 7 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 7 \nNumber of booster rounds = 100\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['mrentabilidad_to_ahorro', 'mrentabilidad_to_sum_fultimo_cierre', 'mrentabilidad_to_margen', 'mrentabilidad_to_sum_mlimitecompra']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 63507 rows and 40 columns with infinity in them...\n        40 variable(s) to be removed since ID or low-information variables\n    \tmore than 40 variables to be removed; too many to print...\ntrain data shape before dropping 40 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 6)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 5 features ############\n#######################################################################################\nSelecting all (5) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 5 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 5 \nNumber of booster rounds = 100\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 2 important features:\n['mrentabilidad_annual_to_ahorro', 'mrentabilidad_annual_to_sum_fultimo_cierre']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 2 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 41195 rows and 37 columns with infinity in them...\n        37 variable(s) to be removed since ID or low-information variables\n    \tmore than 37 variables to be removed; too many to print...\ntrain data shape before dropping 37 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 9)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 8 features ############\n#######################################################################################\nSelecting all (8) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['mcuenta_corriente_to_ahorro', 'mcuenta_corriente_to_sum_mlimitecompra', 'mcuenta_corriente_to_sum_fultimo_cierre', 'mcuenta_corriente_to_sum_Fvencimiento', 'mcuenta_corriente_to_mrentabilidad']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 62232 rows and 39 columns with infinity in them...\n        39 variable(s) to be removed since ID or low-information variables\n    \tmore than 39 variables to be removed; too many to print...\ntrain data shape before dropping 39 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 7)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 6 features ############\n#######################################################################################\nSelecting all (6) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 6 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 6 \nNumber of booster rounds = 100\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 3 important features:\n['mcuentas_saldo_to_sum_fechaalta', 'mcuentas_saldo_to_sum_fultimo_cierre', 'mcuentas_saldo_to_ahorro']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 3 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 4134 rows and 33 columns with infinity in them...\nNo of columns classified 59 does not match 45 total cols. Continuing...\n        47 variable(s) to be removed since ID or low-information variables\n    \tmore than 47 variables to be removed; too many to print...\ntrain data shape before dropping 47 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 13)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 12 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['mplazo_fijo_dolares_to_sum_mlimitecompra']\n    Following (11) vars selected: ['mplazo_fijo_dolares_to_ahorro', 'mplazo_fijo_dolares_to_margen', 'mplazo_fijo_dolares_to_mcuentas_saldo', 'mplazo_fijo_dolares_to_mrentabilidad', 'mplazo_fijo_dolares_to_mrentabilidad_annual', 'mplazo_fijo_dolares_to_sum_Fvencimiento', 'mplazo_fijo_dolares_to_sum_fechaalta', 'mplazo_fijo_dolares_to_sum_fultimo_cierre', 'mplazo_fijo_dolares_to_sum_mpagospesos', 'mplazo_fijo_dolares_to_tarjeta', 'mplazo_fijo_dolares_to_sum_mfinanciacion_limite']\nCompleted SULOV. 11 features selected\nTime taken for SULOV method = 0 seconds\nFinally 11 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 11 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 6 important features:\n['mplazo_fijo_dolares_to_sum_mpagospesos', 'mplazo_fijo_dolares_to_sum_mfinanciacion_limite', 'mplazo_fijo_dolares_to_mcuentas_saldo', 'mplazo_fijo_dolares_to_sum_fultimo_cierre', 'mplazo_fijo_dolares_to_sum_fechaalta', 'mplazo_fijo_dolares_to_tarjeta']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 6 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 82 rows and 29 columns with infinity in them...\nNo of columns classified 63 does not match 45 total cols. Continuing...\n        47 variable(s) to be removed since ID or low-information variables\n    \tmore than 47 variables to be removed; too many to print...\ntrain data shape before dropping 47 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 17)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 16 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (9) highly correlated variables:\n    ['mplazo_fijo_pesos_to_ahorro', 'mplazo_fijo_pesos_to_mrentabilidad', 'mplazo_fijo_pesos_to_mrentabilidad_annual', 'mplazo_fijo_pesos_to_sum_Fvencimiento', 'mplazo_fijo_pesos_to_sum_fechaalta', 'mplazo_fijo_pesos_to_sum_fultimo_cierre', 'mplazo_fijo_pesos_to_sum_mfinanciacion_limite', 'mplazo_fijo_pesos_to_sum_mlimitecompra', 'mplazo_fijo_pesos_to_tarjeta']\n    Following (7) vars selected: ['mplazo_fijo_pesos_to_sum_cconsumos', 'mplazo_fijo_pesos_to_sum_mconsumospesos', 'mplazo_fijo_pesos_to_sum_mconsumototal', 'mplazo_fijo_pesos_to_sum_mpagospesos', 'mplazo_fijo_pesos_to_margen', 'mplazo_fijo_pesos_to_transfer', 'mplazo_fijo_pesos_to_mcuentas_saldo']\nCompleted SULOV. 7 features selected\nTime taken for SULOV method = 0 seconds\nFinally 7 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 7 \nNumber of booster rounds = 100\n            selecting 2 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 3 important features:\n['mplazo_fijo_pesos_to_transfer', 'mplazo_fijo_pesos_to_sum_cconsumos', 'mplazo_fijo_pesos_to_sum_mconsumototal']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 3 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 13210 rows and 35 columns with infinity in them...\nNo of columns classified 50 does not match 45 total cols. Continuing...\n        40 variable(s) to be removed since ID or low-information variables\n    \tmore than 40 variables to be removed; too many to print...\ntrain data shape before dropping 40 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 11)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['mpayroll_to_sum_Fvencimiento', 'mpayroll_to_sum_mfinanciacion_limite']\n    Following (8) vars selected: ['mpayroll_to_ahorro', 'mpayroll_to_margen', 'mpayroll_to_mrentabilidad', 'mpayroll_to_mrentabilidad_annual', 'mpayroll_to_sum_fechaalta', 'mpayroll_to_tarjeta', 'mpayroll_to_sum_fultimo_cierre', 'mpayroll_to_sum_mlimitecompra']\nCompleted SULOV. 8 features selected\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['mpayroll_to_sum_mlimitecompra', 'mpayroll_to_sum_fultimo_cierre', 'mpayroll_to_ahorro', 'mpayroll_to_tarjeta']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 31 rows and 20 columns with infinity in them...\nNo of columns classified 58 does not match 45 total cols. Continuing...\n        42 variable(s) to be removed since ID or low-information variables\n    \tmore than 42 variables to be removed; too many to print...\ntrain data shape before dropping 42 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 17)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 16 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (12) highly correlated variables:\n    ['mpayroll2_to_comisiones', 'mpayroll2_to_margen', 'mpayroll2_to_mcuentas_saldo', 'mpayroll2_to_mpayroll', 'mpayroll2_to_mrentabilidad', 'mpayroll2_to_mrentabilidad_annual', 'mpayroll2_to_sum_Fvencimiento', 'mpayroll2_to_sum_fechaalta', 'mpayroll2_to_sum_fultimo_cierre', 'mpayroll2_to_sum_mlimitecompra', 'mpayroll2_to_tarjeta', 'mpayroll2_to_transfer']\n    Following (4) vars selected: ['mpayroll2_to_ahorro', 'mpayroll2_to_autoservicio', 'mpayroll2_to_mcuenta_corriente', 'mpayroll2_to_sum_mfinanciacion_limite']\nCompleted SULOV. 4 features selected\nTime taken for SULOV method = 0 seconds\nFinally 4 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 4 \nNumber of booster rounds = 100\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 0 important features:\n[]\nTotal Time taken for featurewiz selection = 0 seconds\nOutput contains a list of 0 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 14657 rows and 35 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        38 variable(s) to be removed since ID or low-information variables\n    \tmore than 38 variables to be removed; too many to print...\ntrain data shape before dropping 38 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 11)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['mpagomiscuentas_to_sum_mlimitecompra']\n    Following (9) vars selected: ['mpagomiscuentas_to_ahorro', 'mpagomiscuentas_to_margen', 'mpagomiscuentas_to_mrentabilidad', 'mpagomiscuentas_to_mrentabilidad_annual', 'mpagomiscuentas_to_sum_Fvencimiento', 'mpagomiscuentas_to_sum_fechaalta', 'mpagomiscuentas_to_sum_fultimo_cierre', 'mpagomiscuentas_to_tarjeta', 'mpagomiscuentas_to_sum_mfinanciacion_limite']\nCompleted SULOV. 9 features selected\nTime taken for SULOV method = 0 seconds\nFinally 9 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 9 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['mpagomiscuentas_to_sum_mfinanciacion_limite', 'mpagomiscuentas_to_sum_fultimo_cierre', 'mpagomiscuentas_to_sum_fechaalta', 'mpagomiscuentas_to_tarjeta']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 889 rows and 32 columns with infinity in them...\nNo of columns classified 63 does not match 45 total cols. Continuing...\n        50 variable(s) to be removed since ID or low-information variables\n    \tmore than 50 variables to be removed; too many to print...\ntrain data shape before dropping 50 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 14)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 13 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['mcajeros_propios_descuentos_to_sum_Fvencimiento', 'mcajeros_propios_descuentos_to_sum_mfinanciacion_limite']\n    Following (11) vars selected: ['mcajeros_propios_descuentos_to_ahorro', 'mcajeros_propios_descuentos_to_autoservicio', 'mcajeros_propios_descuentos_to_comisiones', 'mcajeros_propios_descuentos_to_margen', 'mcajeros_propios_descuentos_to_mcuentas_saldo', 'mcajeros_propios_descuentos_to_mrentabilidad', 'mcajeros_propios_descuentos_to_mrentabilidad_annual', 'mcajeros_propios_descuentos_to_sum_fechaalta', 'mcajeros_propios_descuentos_to_tarjeta', 'mcajeros_propios_descuentos_to_sum_mlimitecompra', 'mcajeros_propios_descuentos_to_sum_fultimo_cierre']\nCompleted SULOV. 11 features selected\nTime taken for SULOV method = 0 seconds\nFinally 11 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 11 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 7 important features:\n['mcajeros_propios_descuentos_to_autoservicio', 'mcajeros_propios_descuentos_to_sum_mlimitecompra', 'mcajeros_propios_descuentos_to_sum_fultimo_cierre', 'mcajeros_propios_descuentos_to_mcuentas_saldo', 'mcajeros_propios_descuentos_to_ahorro', 'mcajeros_propios_descuentos_to_comisiones', 'mcajeros_propios_descuentos_to_mrentabilidad']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 7 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 1483 rows and 33 columns with infinity in them...\nNo of columns classified 63 does not match 45 total cols. Continuing...\n        51 variable(s) to be removed since ID or low-information variables\n    \tmore than 51 variables to be removed; too many to print...\ntrain data shape before dropping 51 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 13)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 12 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (4) highly correlated variables:\n    ['sum_delinquency_to_ahorro', 'sum_delinquency_to_sum_Fvencimiento', 'sum_delinquency_to_sum_mpagominimo', 'sum_delinquency_to_sum_msaldopesos']\n    Following (8) vars selected: ['sum_delinquency_to_margen', 'sum_delinquency_to_mrentabilidad', 'sum_delinquency_to_mrentabilidad_annual', 'sum_delinquency_to_sum_fechaalta', 'sum_delinquency_to_sum_mlimitecompra', 'sum_delinquency_to_tarjeta', 'sum_delinquency_to_sum_msaldototal', 'sum_delinquency_to_sum_fultimo_cierre']\nCompleted SULOV. 8 features selected\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['sum_delinquency_to_sum_msaldototal', 'sum_delinquency_to_sum_fultimo_cierre', 'sum_delinquency_to_margen', 'sum_delinquency_to_mrentabilidad', 'sum_delinquency_to_tarjeta']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 5962 rows and 36 columns with infinity in them...\nNo of columns classified 57 does not match 45 total cols. Continuing...\n        48 variable(s) to be removed since ID or low-information variables\n    \tmore than 48 variables to be removed; too many to print...\ntrain data shape before dropping 48 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 10)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 9 features ############\n#######################################################################################\nSelecting all (9) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 9 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 9 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 6 important features:\n['sum_status_to_ahorro', 'sum_status_to_sum_fultimo_cierre', 'sum_status_to_sum_Fvencimiento', 'sum_status_to_tarjeta', 'sum_status_to_sum_fechaalta', 'sum_status_to_sum_mlimitecompra']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 6 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 51555 rows and 39 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        42 variable(s) to be removed since ID or low-information variables\n    \tmore than 42 variables to be removed; too many to print...\ntrain data shape before dropping 42 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 7)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 6 features ############\n#######################################################################################\nSelecting all (6) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 6 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 6 \nNumber of booster rounds = 100\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['sum_mfinanciacion_limite_to_ahorro', 'sum_mfinanciacion_limite_to_tarjeta', 'sum_mfinanciacion_limite_to_sum_fechaalta', 'sum_mfinanciacion_limite_to_sum_fultimo_cierre']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 51685 rows and 40 columns with infinity in them...\n        40 variable(s) to be removed since ID or low-information variables\n    \tmore than 40 variables to be removed; too many to print...\ntrain data shape before dropping 40 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 6)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 5 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['sum_Fvencimiento_to_sum_mlimitecompra']\n    Following (4) vars selected: ['sum_Fvencimiento_to_sum_fechaalta', 'sum_Fvencimiento_to_sum_fultimo_cierre', 'sum_Fvencimiento_to_tarjeta', 'sum_Fvencimiento_to_ahorro']\nCompleted SULOV. 4 features selected\nTime taken for SULOV method = 0 seconds\nFinally 4 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 4 \nNumber of booster rounds = 100\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 2 important features:\n['sum_Fvencimiento_to_ahorro', 'sum_Fvencimiento_to_tarjeta']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 2 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 40784 rows and 36 columns with infinity in them...\n        36 variable(s) to be removed since ID or low-information variables\n    \tmore than 36 variables to be removed; too many to print...\ntrain data shape before dropping 36 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 10)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 9 features ############\n#######################################################################################\nSelecting all (9) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 9 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 9 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['sum_msaldototal_to_sum_fultimo_cierre', 'sum_msaldototal_to_ahorro', 'sum_msaldototal_to_sum_Fvencimiento', 'sum_msaldototal_to_sum_fechaalta', 'sum_msaldototal_to_tarjeta']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 40720 rows and 35 columns with infinity in them...\n        35 variable(s) to be removed since ID or low-information variables\n    \tmore than 35 variables to be removed; too many to print...\ntrain data shape before dropping 35 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 11)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n#######################################################################################\nSelecting all (10) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 10 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 10 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 6 important features:\n['sum_msaldopesos_to_sum_fultimo_cierre', 'sum_msaldopesos_to_ahorro', 'sum_msaldopesos_to_sum_Fvencimiento', 'sum_msaldopesos_to_sum_fechaalta', 'sum_msaldopesos_to_sum_mlimitecompra', 'sum_msaldopesos_to_tarjeta']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 6 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 4709 rows and 33 columns with infinity in them...\nNo of columns classified 58 does not match 45 total cols. Continuing...\n        46 variable(s) to be removed since ID or low-information variables\n    \tmore than 46 variables to be removed; too many to print...\ntrain data shape before dropping 46 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 13)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 12 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['sum_msaldodolares_to_sum_fultimo_cierre', 'sum_msaldodolares_to_sum_mfinanciacion_limite']\n    Following (10) vars selected: ['sum_msaldodolares_to_ahorro', 'sum_msaldodolares_to_margen', 'sum_msaldodolares_to_mrentabilidad', 'sum_msaldodolares_to_mrentabilidad_annual', 'sum_msaldodolares_to_sum_fechaalta', 'sum_msaldodolares_to_sum_mpagospesos', 'sum_msaldodolares_to_sum_msaldototal', 'sum_msaldodolares_to_tarjeta', 'sum_msaldodolares_to_sum_mlimitecompra', 'sum_msaldodolares_to_sum_Fvencimiento']\nCompleted SULOV. 10 features selected\nTime taken for SULOV method = 0 seconds\nFinally 10 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 10 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['sum_msaldodolares_to_sum_msaldototal', 'sum_msaldodolares_to_sum_mpagospesos', 'sum_msaldodolares_to_sum_fechaalta', 'sum_msaldodolares_to_ahorro', 'sum_msaldodolares_to_sum_Fvencimiento']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 14999 rows and 31 columns with infinity in them...\n        32 variable(s) to be removed since ID or low-information variables\n    \tmore than 32 variables to be removed; too many to print...\ntrain data shape before dropping 32 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 14)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 13 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (3) highly correlated variables:\n    ['sum_mconsumospesos_to_sum_fultimo_cierre', 'sum_mconsumospesos_to_sum_mlimitecompra', 'sum_mconsumospesos_to_sum_msaldopesos']\n    Following (10) vars selected: ['sum_mconsumospesos_to_margen', 'sum_mconsumospesos_to_mrentabilidad', 'sum_mconsumospesos_to_mrentabilidad_annual', 'sum_mconsumospesos_to_sum_Fvencimiento', 'sum_mconsumospesos_to_sum_cconsumos', 'sum_mconsumospesos_to_sum_fechaalta', 'sum_mconsumospesos_to_tarjeta', 'sum_mconsumospesos_to_ahorro', 'sum_mconsumospesos_to_sum_mfinanciacion_limite', 'sum_mconsumospesos_to_sum_msaldototal']\nCompleted SULOV. 10 features selected\nTime taken for SULOV method = 0 seconds\nFinally 10 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 10 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['sum_mconsumospesos_to_sum_fechaalta', 'sum_mconsumospesos_to_ahorro', 'sum_mconsumospesos_to_sum_mfinanciacion_limite', 'sum_mconsumospesos_to_sum_Fvencimiento', 'sum_mconsumospesos_to_tarjeta']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 4786 rows and 28 columns with infinity in them...\nNo of columns classified 62 does not match 45 total cols. Continuing...\n        45 variable(s) to be removed since ID or low-information variables\n    \tmore than 45 variables to be removed; too many to print...\ntrain data shape before dropping 45 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 18)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 17 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (6) highly correlated variables:\n    ['sum_mconsumosdolares_to_sum_Fvencimiento', 'sum_mconsumosdolares_to_sum_fechaalta', 'sum_mconsumosdolares_to_sum_fultimo_cierre', 'sum_mconsumosdolares_to_sum_mlimitecompra', 'sum_mconsumosdolares_to_sum_msaldopesos', 'sum_mconsumosdolares_to_tarjeta']\n    Following (11) vars selected: ['sum_mconsumosdolares_to_margen', 'sum_mconsumosdolares_to_mcuentas_saldo', 'sum_mconsumosdolares_to_sum_cconsumos', 'sum_mconsumosdolares_to_sum_mconsumospesos', 'sum_mconsumosdolares_to_sum_mconsumototal', 'sum_mconsumosdolares_to_sum_mpagominimo', 'sum_mconsumosdolares_to_ahorro', 'sum_mconsumosdolares_to_mrentabilidad_annual', 'sum_mconsumosdolares_to_sum_mfinanciacion_limite', 'sum_mconsumosdolares_to_sum_msaldototal', 'sum_mconsumosdolares_to_mrentabilidad']\nCompleted SULOV. 11 features selected\nTime taken for SULOV method = 0 seconds\nFinally 11 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 11 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 9 important features:\n['sum_mconsumosdolares_to_sum_mconsumospesos', 'sum_mconsumosdolares_to_sum_cconsumos', 'sum_mconsumosdolares_to_mcuentas_saldo', 'sum_mconsumosdolares_to_ahorro', 'sum_mconsumosdolares_to_mrentabilidad', 'sum_mconsumosdolares_to_mrentabilidad_annual', 'sum_mconsumosdolares_to_sum_mconsumototal', 'sum_mconsumosdolares_to_sum_mfinanciacion_limite', 'sum_mconsumosdolares_to_sum_msaldototal']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 9 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 51685 rows and 40 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        43 variable(s) to be removed since ID or low-information variables\n    \tmore than 43 variables to be removed; too many to print...\ntrain data shape before dropping 43 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 6)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 5 features ############\n#######################################################################################\nSelecting all (5) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 5 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 5 \nNumber of booster rounds = 100\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 2 important features:\n['sum_mlimitecompra_to_ahorro', 'sum_mlimitecompra_to_tarjeta']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 2 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 55 rows and 23 columns with infinity in them...\nNo of columns classified 65 does not match 45 total cols. Continuing...\n        45 variable(s) to be removed since ID or low-information variables\n    \tmore than 45 variables to be removed; too many to print...\ntrain data shape before dropping 45 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 21)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (16) highly correlated variables:\n    ['sum_madelantopesos_to_ahorro', 'sum_madelantopesos_to_comisiones', 'sum_madelantopesos_to_margen', 'sum_madelantopesos_to_mcuenta_corriente', 'sum_madelantopesos_to_mrentabilidad_annual', 'sum_madelantopesos_to_sum_Fvencimiento', 'sum_madelantopesos_to_sum_cconsumos', 'sum_madelantopesos_to_sum_fechaalta', 'sum_madelantopesos_to_sum_fultimo_cierre', 'sum_madelantopesos_to_sum_mconsumototal', 'sum_madelantopesos_to_sum_mfinanciacion_limite', 'sum_madelantopesos_to_sum_mpagominimo', 'sum_madelantopesos_to_sum_mpagospesos', 'sum_madelantopesos_to_sum_msaldopesos', 'sum_madelantopesos_to_sum_msaldototal', 'sum_madelantopesos_to_tarjeta']\n    Following (4) vars selected: ['sum_madelantopesos_to_mrentabilidad', 'sum_madelantopesos_to_sum_mconsumospesos', 'sum_madelantopesos_to_mcuentas_saldo', 'sum_madelantopesos_to_sum_mlimitecompra']\nCompleted SULOV. 4 features selected\nTime taken for SULOV method = 0 seconds\nFinally 4 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 4 \nNumber of booster rounds = 100\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 0 important features:\n[]\nTotal Time taken for featurewiz selection = 0 seconds\nOutput contains a list of 0 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 51637 rows and 40 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        43 variable(s) to be removed since ID or low-information variables\n    \tmore than 43 variables to be removed; too many to print...\ntrain data shape before dropping 43 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 6)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 5 features ############\n#######################################################################################\nSelecting all (5) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 5 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 5 \nNumber of booster rounds = 100\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 2 important features:\n['sum_fultimo_cierre_to_ahorro', 'sum_fultimo_cierre_to_tarjeta']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 2 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 4444 rows and 33 columns with infinity in them...\nNo of columns classified 58 does not match 45 total cols. Continuing...\n        46 variable(s) to be removed since ID or low-information variables\n    \tmore than 46 variables to be removed; too many to print...\ntrain data shape before dropping 46 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 13)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 12 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['sum_mpagado_to_sum_mfinanciacion_limite', 'sum_mpagado_to_sum_msaldototal']\n    Following (10) vars selected: ['sum_mpagado_to_ahorro', 'sum_mpagado_to_margen', 'sum_mpagado_to_mrentabilidad', 'sum_mpagado_to_mrentabilidad_annual', 'sum_mpagado_to_sum_Fvencimiento', 'sum_mpagado_to_sum_fechaalta', 'sum_mpagado_to_sum_fultimo_cierre', 'sum_mpagado_to_tarjeta', 'sum_mpagado_to_sum_msaldopesos', 'sum_mpagado_to_sum_mlimitecompra']\nCompleted SULOV. 10 features selected\nTime taken for SULOV method = 0 seconds\nFinally 10 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 10 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 6 important features:\n['sum_mpagado_to_sum_msaldopesos', 'sum_mpagado_to_ahorro', 'sum_mpagado_to_sum_Fvencimiento', 'sum_mpagado_to_sum_fultimo_cierre', 'sum_mpagado_to_sum_fechaalta', 'sum_mpagado_to_sum_mlimitecompra']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 6 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 16120 rows and 35 columns with infinity in them...\nNo of columns classified 49 does not match 45 total cols. Continuing...\n        39 variable(s) to be removed since ID or low-information variables\n    \tmore than 39 variables to be removed; too many to print...\ntrain data shape before dropping 39 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 11)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['sum_mpagospesos_to_sum_mlimitecompra']\n    Following (9) vars selected: ['sum_mpagospesos_to_ahorro', 'sum_mpagospesos_to_margen', 'sum_mpagospesos_to_mrentabilidad', 'sum_mpagospesos_to_mrentabilidad_annual', 'sum_mpagospesos_to_sum_Fvencimiento', 'sum_mpagospesos_to_sum_fechaalta', 'sum_mpagospesos_to_sum_fultimo_cierre', 'sum_mpagospesos_to_tarjeta', 'sum_mpagospesos_to_sum_mfinanciacion_limite']\nCompleted SULOV. 9 features selected\nTime taken for SULOV method = 0 seconds\nFinally 9 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 9 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['sum_mpagospesos_to_sum_fechaalta', 'sum_mpagospesos_to_sum_Fvencimiento', 'sum_mpagospesos_to_ahorro', 'sum_mpagospesos_to_sum_fultimo_cierre', 'sum_mpagospesos_to_tarjeta']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 1920 rows and 26 columns with infinity in them...\nNo of columns classified 64 does not match 45 total cols. Continuing...\n        45 variable(s) to be removed since ID or low-information variables\n    \tmore than 45 variables to be removed; too many to print...\ntrain data shape before dropping 45 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 20)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 19 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (8) highly correlated variables:\n    ['sum_mpagosdolares_to_comisiones', 'sum_mpagosdolares_to_mrentabilidad_annual', 'sum_mpagosdolares_to_sum_Fvencimiento', 'sum_mpagosdolares_to_sum_fechaalta', 'sum_mpagosdolares_to_sum_mlimitecompra', 'sum_mpagosdolares_to_sum_mpagominimo', 'sum_mpagosdolares_to_sum_mpagospesos', 'sum_mpagosdolares_to_sum_msaldopesos']\n    Following (11) vars selected: ['sum_mpagosdolares_to_ahorro', 'sum_mpagosdolares_to_mcuentas_saldo', 'sum_mpagosdolares_to_mrentabilidad', 'sum_mpagosdolares_to_sum_mconsumospesos', 'sum_mpagosdolares_to_sum_mconsumototal', 'sum_mpagosdolares_to_tarjeta', 'sum_mpagosdolares_to_sum_fultimo_cierre', 'sum_mpagosdolares_to_sum_mfinanciacion_limite', 'sum_mpagosdolares_to_margen', 'sum_mpagosdolares_to_sum_cconsumos', 'sum_mpagosdolares_to_sum_msaldototal']\nCompleted SULOV. 11 features selected\nTime taken for SULOV method = 0 seconds\nFinally 11 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 11 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 9 important features:\n['sum_mpagosdolares_to_sum_mconsumospesos', 'sum_mpagosdolares_to_mcuentas_saldo', 'sum_mpagosdolares_to_ahorro', 'sum_mpagosdolares_to_margen', 'sum_mpagosdolares_to_sum_cconsumos', 'sum_mpagosdolares_to_mrentabilidad', 'sum_mpagosdolares_to_sum_fultimo_cierre', 'sum_mpagosdolares_to_sum_mconsumototal', 'sum_mpagosdolares_to_tarjeta']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 9 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 51685 rows and 40 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        43 variable(s) to be removed since ID or low-information variables\n    \tmore than 43 variables to be removed; too many to print...\ntrain data shape before dropping 43 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 6)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 5 features ############\n#######################################################################################\nSelecting all (5) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 5 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 5 \nNumber of booster rounds = 100\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 2 important features:\n['sum_fechaalta_to_ahorro', 'sum_fechaalta_to_tarjeta']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 2 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 14999 rows and 31 columns with infinity in them...\n        32 variable(s) to be removed since ID or low-information variables\n    \tmore than 32 variables to be removed; too many to print...\ntrain data shape before dropping 32 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 14)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 13 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (3) highly correlated variables:\n    ['sum_mconsumototal_to_sum_fultimo_cierre', 'sum_mconsumototal_to_sum_mlimitecompra', 'sum_mconsumototal_to_sum_msaldopesos']\n    Following (10) vars selected: ['sum_mconsumototal_to_margen', 'sum_mconsumototal_to_mrentabilidad', 'sum_mconsumototal_to_mrentabilidad_annual', 'sum_mconsumototal_to_sum_Fvencimiento', 'sum_mconsumototal_to_sum_cconsumos', 'sum_mconsumototal_to_sum_fechaalta', 'sum_mconsumototal_to_tarjeta', 'sum_mconsumototal_to_ahorro', 'sum_mconsumototal_to_sum_mfinanciacion_limite', 'sum_mconsumototal_to_sum_msaldototal']\nCompleted SULOV. 10 features selected\nTime taken for SULOV method = 0 seconds\nFinally 10 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 10 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['sum_mconsumototal_to_sum_fechaalta', 'sum_mconsumototal_to_ahorro', 'sum_mconsumototal_to_sum_mfinanciacion_limite', 'sum_mconsumototal_to_sum_Fvencimiento', 'sum_mconsumototal_to_tarjeta']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 14999 rows and 31 columns with infinity in them...\nNo of columns classified 49 does not match 45 total cols. Continuing...\n        35 variable(s) to be removed since ID or low-information variables\n    \tmore than 35 variables to be removed; too many to print...\ntrain data shape before dropping 35 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 15)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 14 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['sum_cconsumos_to_sum_mlimitecompra', 'sum_cconsumos_to_sum_msaldototal']\n    Following (12) vars selected: ['sum_cconsumos_to_ahorro', 'sum_cconsumos_to_margen', 'sum_cconsumos_to_mrentabilidad', 'sum_cconsumos_to_mrentabilidad_annual', 'sum_cconsumos_to_sum_Fvencimiento', 'sum_cconsumos_to_sum_fechaalta', 'sum_cconsumos_to_sum_fultimo_cierre', 'sum_cconsumos_to_sum_mconsumospesos', 'sum_cconsumos_to_sum_mconsumototal', 'sum_cconsumos_to_tarjeta', 'sum_cconsumos_to_sum_msaldopesos', 'sum_cconsumos_to_sum_mfinanciacion_limite']\nCompleted SULOV. 12 features selected\nTime taken for SULOV method = 0 seconds\nFinally 12 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 12 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 7 important features:\n['sum_cconsumos_to_sum_fechaalta', 'sum_cconsumos_to_sum_mconsumospesos', 'sum_cconsumos_to_ahorro', 'sum_cconsumos_to_sum_fultimo_cierre', 'sum_cconsumos_to_sum_Fvencimiento', 'sum_cconsumos_to_sum_mconsumototal', 'sum_cconsumos_to_sum_msaldopesos']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 7 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 55 rows and 23 columns with infinity in them...\nNo of columns classified 65 does not match 45 total cols. Continuing...\n        45 variable(s) to be removed since ID or low-information variables\n    \tmore than 45 variables to be removed; too many to print...\ntrain data shape before dropping 45 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 21)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (15) highly correlated variables:\n    ['sum_cadelantosefectivo_to_ahorro', 'sum_cadelantosefectivo_to_comisiones', 'sum_cadelantosefectivo_to_margen', 'sum_cadelantosefectivo_to_mcuentas_saldo', 'sum_cadelantosefectivo_to_mrentabilidad', 'sum_cadelantosefectivo_to_mrentabilidad_annual', 'sum_cadelantosefectivo_to_sum_Fvencimiento', 'sum_cadelantosefectivo_to_sum_cconsumos', 'sum_cadelantosefectivo_to_sum_fultimo_cierre', 'sum_cadelantosefectivo_to_sum_mfinanciacion_limite', 'sum_cadelantosefectivo_to_sum_mpagominimo', 'sum_cadelantosefectivo_to_sum_mpagospesos', 'sum_cadelantosefectivo_to_sum_msaldopesos', 'sum_cadelantosefectivo_to_sum_msaldototal', 'sum_cadelantosefectivo_to_tarjeta']\n    Following (5) vars selected: ['sum_cadelantosefectivo_to_sum_fechaalta', 'sum_cadelantosefectivo_to_sum_mconsumospesos', 'sum_cadelantosefectivo_to_sum_mlimitecompra', 'sum_cadelantosefectivo_to_sum_mconsumototal', 'sum_cadelantosefectivo_to_mcuenta_corriente']\nCompleted SULOV. 5 features selected\nTime taken for SULOV method = 0 seconds\nFinally 5 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 5 \nNumber of booster rounds = 100\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 0 important features:\n[]\nTotal Time taken for featurewiz selection = 0 seconds\nOutput contains a list of 0 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 34966 rows and 34 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        37 variable(s) to be removed since ID or low-information variables\n    \tmore than 37 variables to be removed; too many to print...\ntrain data shape before dropping 37 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 12)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 11 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['sum_mpagominimo_to_sum_msaldototal']\n    Following (10) vars selected: ['sum_mpagominimo_to_ahorro', 'sum_mpagominimo_to_margen', 'sum_mpagominimo_to_mrentabilidad', 'sum_mpagominimo_to_mrentabilidad_annual', 'sum_mpagominimo_to_sum_Fvencimiento', 'sum_mpagominimo_to_sum_fechaalta', 'sum_mpagominimo_to_sum_fultimo_cierre', 'sum_mpagominimo_to_sum_mlimitecompra', 'sum_mpagominimo_to_tarjeta', 'sum_mpagominimo_to_sum_msaldopesos']\nCompleted SULOV. 10 features selected\nTime taken for SULOV method = 0 seconds\nFinally 10 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 10 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 6 important features:\n['sum_mpagominimo_to_sum_fechaalta', 'sum_mpagominimo_to_ahorro', 'sum_mpagominimo_to_sum_fultimo_cierre', 'sum_mpagominimo_to_tarjeta', 'sum_mpagominimo_to_sum_Fvencimiento', 'sum_mpagominimo_to_sum_mlimitecompra']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 6 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 14125 rows and 37 columns with infinity in them...\nNo of columns classified 51 does not match 45 total cols. Continuing...\n        43 variable(s) to be removed since ID or low-information variables\n    \tmore than 43 variables to be removed; too many to print...\ntrain data shape before dropping 43 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 9)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 8 features ############\n#######################################################################################\nSelecting all (8) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['prestamo_to_sum_fechaalta', 'prestamo_to_sum_Fvencimiento', 'prestamo_to_ahorro', 'prestamo_to_sum_fultimo_cierre']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 194 rows and 28 columns with infinity in them...\nNo of columns classified 63 does not match 45 total cols. Continuing...\n        46 variable(s) to be removed since ID or low-information variables\n    \tmore than 46 variables to be removed; too many to print...\ntrain data shape before dropping 46 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 18)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 17 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (9) highly correlated variables:\n    ['servicios_to_comisiones', 'servicios_to_margen', 'servicios_to_mrentabilidad', 'servicios_to_mrentabilidad_annual', 'servicios_to_sum_Fvencimiento', 'servicios_to_sum_fechaalta', 'servicios_to_sum_mfinanciacion_limite', 'servicios_to_sum_mlimitecompra', 'servicios_to_tarjeta']\n    Following (8) vars selected: ['servicios_to_ahorro', 'servicios_to_sum_cconsumos', 'servicios_to_sum_mconsumospesos', 'servicios_to_sum_mconsumototal', 'servicios_to_sum_mpagospesos', 'servicios_to_sum_fultimo_cierre', 'servicios_to_mcuentas_saldo', 'servicios_to_autoservicio']\nCompleted SULOV. 8 features selected\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['servicios_to_autoservicio', 'servicios_to_sum_cconsumos', 'servicios_to_sum_fultimo_cierre', 'servicios_to_sum_mconsumospesos', 'servicios_to_sum_mpagospesos']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 57555 rows and 36 columns with infinity in them...\n        36 variable(s) to be removed since ID or low-information variables\n    \tmore than 36 variables to be removed; too many to print...\ntrain data shape before dropping 36 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 10)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 9 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['comisiones_to_sum_mfinanciacion_limite']\n    Following (8) vars selected: ['comisiones_to_ahorro', 'comisiones_to_margen', 'comisiones_to_mrentabilidad', 'comisiones_to_mrentabilidad_annual', 'comisiones_to_sum_Fvencimiento', 'comisiones_to_sum_fechaalta', 'comisiones_to_sum_fultimo_cierre', 'comisiones_to_sum_mlimitecompra']\nCompleted SULOV. 8 features selected\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['comisiones_to_ahorro', 'comisiones_to_sum_fechaalta', 'comisiones_to_sum_mlimitecompra', 'comisiones_to_sum_fultimo_cierre', 'comisiones_to_mrentabilidad']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 1488 rows and 33 columns with infinity in them...\nNo of columns classified 63 does not match 45 total cols. Continuing...\n        51 variable(s) to be removed since ID or low-information variables\n    \tmore than 51 variables to be removed; too many to print...\ntrain data shape before dropping 51 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 13)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 12 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (4) highly correlated variables:\n    ['cheques_to_comisiones', 'cheques_to_sum_fultimo_cierre', 'cheques_to_sum_mfinanciacion_limite', 'cheques_to_sum_mlimitecompra']\n    Following (8) vars selected: ['cheques_to_ahorro', 'cheques_to_margen', 'cheques_to_mcuentas_saldo', 'cheques_to_mrentabilidad', 'cheques_to_mrentabilidad_annual', 'cheques_to_tarjeta', 'cheques_to_sum_fechaalta', 'cheques_to_sum_Fvencimiento']\nCompleted SULOV. 8 features selected\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['cheques_to_sum_fechaalta', 'cheques_to_sum_Fvencimiento', 'cheques_to_mcuentas_saldo', 'cheques_to_ahorro']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 63793 rows and 41 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        44 variable(s) to be removed since ID or low-information variables\n    \tmore than 44 variables to be removed; too many to print...\ntrain data shape before dropping 44 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 5)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 4 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['ahorro_to_sum_Fvencimiento']\n    Following (3) vars selected: ['ahorro_to_sum_fechaalta', 'ahorro_to_sum_mlimitecompra', 'ahorro_to_sum_fultimo_cierre']\nCompleted SULOV. 3 features selected\nTime taken for SULOV method = 0 seconds\nFinally 3 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 3 \nNumber of booster rounds = 100\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 1 important features:\n['ahorro_to_sum_fultimo_cierre']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 1 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 2467 rows and 37 columns with infinity in them...\nNo of columns classified 59 does not match 45 total cols. Continuing...\n        51 variable(s) to be removed since ID or low-information variables\n    \tmore than 51 variables to be removed; too many to print...\ntrain data shape before dropping 51 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 9)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 8 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['inversion_to_mrentabilidad_annual', 'inversion_to_sum_mlimitecompra']\n    Following (6) vars selected: ['inversion_to_ahorro', 'inversion_to_sum_Fvencimiento', 'inversion_to_sum_fechaalta', 'inversion_to_tarjeta', 'inversion_to_sum_fultimo_cierre', 'inversion_to_sum_mfinanciacion_limite']\nCompleted SULOV. 6 features selected\nTime taken for SULOV method = 0 seconds\nFinally 6 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 6 \nNumber of booster rounds = 100\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 3 important features:\n['inversion_to_sum_mfinanciacion_limite', 'inversion_to_sum_Fvencimiento', 'inversion_to_sum_fultimo_cierre']\nTotal Time taken for featurewiz selection = 0 seconds\nOutput contains a list of 3 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 63653 rows and 40 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        43 variable(s) to be removed since ID or low-information variables\n    \tmore than 43 variables to be removed; too many to print...\ntrain data shape before dropping 43 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 6)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 5 features ############\n#######################################################################################\nSelecting all (5) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 5 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 5 \nNumber of booster rounds = 100\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 2 important features:\n['tarjeta_to_sum_fechaalta', 'tarjeta_to_sum_fultimo_cierre']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 2 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 63372 rows and 38 columns with infinity in them...\n        38 variable(s) to be removed since ID or low-information variables\n    \tmore than 38 variables to be removed; too many to print...\ntrain data shape before dropping 38 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 8)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 7 features ############\n#######################################################################################\nSelecting all (7) variables since none of numeric vars are highly correlated...\nTime taken for SULOV method = 0 seconds\nFinally 7 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 7 \nNumber of booster rounds = 100\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['margen_to_ahorro', 'margen_to_sum_mlimitecompra', 'margen_to_sum_fultimo_cierre', 'margen_to_sum_Fvencimiento']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 16140 rows and 35 columns with infinity in them...\nNo of columns classified 50 does not match 45 total cols. Continuing...\n        40 variable(s) to be removed since ID or low-information variables\n    \tmore than 40 variables to be removed; too many to print...\ntrain data shape before dropping 40 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 11)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['debit_to_sum_fultimo_cierre', 'debit_to_sum_mfinanciacion_limite']\n    Following (8) vars selected: ['debit_to_ahorro', 'debit_to_margen', 'debit_to_mrentabilidad', 'debit_to_mrentabilidad_annual', 'debit_to_sum_fechaalta', 'debit_to_tarjeta', 'debit_to_sum_Fvencimiento', 'debit_to_sum_mlimitecompra']\nCompleted SULOV. 8 features selected\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['debit_to_sum_fechaalta', 'debit_to_sum_Fvencimiento', 'debit_to_ahorro', 'debit_to_tarjeta']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 2082 rows and 33 columns with infinity in them...\nNo of columns classified 60 does not match 45 total cols. Continuing...\n        48 variable(s) to be removed since ID or low-information variables\n    \tmore than 48 variables to be removed; too many to print...\ntrain data shape before dropping 48 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 13)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 12 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['forex_to_sum_mlimitecompra']\n    Following (11) vars selected: ['forex_to_ahorro', 'forex_to_comisiones', 'forex_to_margen', 'forex_to_mcuentas_saldo', 'forex_to_mrentabilidad', 'forex_to_mrentabilidad_annual', 'forex_to_sum_Fvencimiento', 'forex_to_sum_fechaalta', 'forex_to_sum_fultimo_cierre', 'forex_to_tarjeta', 'forex_to_sum_mfinanciacion_limite']\nCompleted SULOV. 11 features selected\nTime taken for SULOV method = 0 seconds\nFinally 11 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 11 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 5 important features:\n['forex_to_sum_mfinanciacion_limite', 'forex_to_sum_Fvencimiento', 'forex_to_comisiones', 'forex_to_sum_fechaalta', 'forex_to_mcuentas_saldo']\nTotal Time taken for featurewiz selection = 1 seconds\nOutput contains a list of 5 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 35303 rows and 35 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        38 variable(s) to be removed since ID or low-information variables\n    \tmore than 38 variables to be removed; too many to print...\ntrain data shape before dropping 38 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 11)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['transfer_to_sum_fultimo_cierre', 'transfer_to_sum_mlimitecompra']\n    Following (8) vars selected: ['transfer_to_ahorro', 'transfer_to_margen', 'transfer_to_mrentabilidad', 'transfer_to_mrentabilidad_annual', 'transfer_to_sum_fechaalta', 'transfer_to_tarjeta', 'transfer_to_sum_Fvencimiento', 'transfer_to_sum_mfinanciacion_limite']\nCompleted SULOV. 8 features selected\nTime taken for SULOV method = 0 seconds\nFinally 8 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 8 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['transfer_to_sum_Fvencimiento', 'transfer_to_sum_fechaalta', 'transfer_to_tarjeta', 'transfer_to_ahorro']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 25613 rows and 35 columns with infinity in them...\nNo of columns classified 48 does not match 45 total cols. Continuing...\n        38 variable(s) to be removed since ID or low-information variables\n    \tmore than 38 variables to be removed; too many to print...\ntrain data shape before dropping 38 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 11)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['autoservicio_to_sum_mfinanciacion_limite']\n    Following (9) vars selected: ['autoservicio_to_ahorro', 'autoservicio_to_margen', 'autoservicio_to_mrentabilidad', 'autoservicio_to_mrentabilidad_annual', 'autoservicio_to_sum_Fvencimiento', 'autoservicio_to_sum_fechaalta', 'autoservicio_to_sum_fultimo_cierre', 'autoservicio_to_tarjeta', 'autoservicio_to_sum_mlimitecompra']\nCompleted SULOV. 9 features selected\nTime taken for SULOV method = 0 seconds\nFinally 9 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 9 \nNumber of booster rounds = 100\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n            selecting 1 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 4 important features:\n['autoservicio_to_sum_Fvencimiento', 'autoservicio_to_sum_fultimo_cierre', 'autoservicio_to_ahorro', 'autoservicio_to_tarjeta']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 4 important features and a train dataframe\n############################################################################################\n############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n############################################################################################\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (3234, 46)\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n    there are 15177 rows and 34 columns with infinity in them...\nNo of columns classified 50 does not match 45 total cols. Continuing...\n        39 variable(s) to be removed since ID or low-information variables\n    \tmore than 39 variables to be removed; too many to print...\ntrain data shape before dropping 39 columns = (3234, 46)\n\ttrain data shape after dropping columns = (3234, 12)\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 11 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['atm_to_sum_mlimitecompra']\n    Following (10) vars selected: ['atm_to_ahorro', 'atm_to_autoservicio', 'atm_to_margen', 'atm_to_mrentabilidad', 'atm_to_mrentabilidad_annual', 'atm_to_sum_Fvencimiento', 'atm_to_sum_fechaalta', 'atm_to_sum_fultimo_cierre', 'atm_to_tarjeta', 'atm_to_sum_mfinanciacion_limite']\nCompleted SULOV. 10 features selected\nTime taken for SULOV method = 0 seconds\nFinally 10 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n#######################################################################################\n#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n#######################################################################################\nCurrent number of predictors before recursive XGBoost = 10 \nNumber of booster rounds = 100\n            selecting 5 features in this iteration\n            selecting 4 features in this iteration\n            selecting 3 features in this iteration\n            selecting 2 features in this iteration\n    Completed XGBoost feature selection in 0 seconds\n#######################################################################################\n#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n#######################################################################################\nSelected 6 important features:\n['atm_to_autoservicio', 'atm_to_sum_Fvencimiento', 'atm_to_sum_fechaalta', 'atm_to_sum_fultimo_cierre', 'atm_to_tarjeta', 'atm_to_sum_mfinanciacion_limite']\nTotal Time taken for featurewiz selection = 2 seconds\nOutput contains a list of 6 important features and a train dataframe\n","output_type":"stream"}]},{"cell_type":"code","source":"best_unb","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:12:19.840913Z","iopub.execute_input":"2023-04-10T01:12:19.841661Z","iopub.status.idle":"2023-04-10T01:12:19.852811Z","shell.execute_reply.started":"2023-04-10T01:12:19.841622Z","shell.execute_reply":"2023-04-10T01:12:19.851515Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[['mrentabilidad_to_ahorro',\n  'mrentabilidad_to_sum_fultimo_cierre',\n  'mrentabilidad_to_margen',\n  'mrentabilidad_to_sum_mlimitecompra'],\n ['mrentabilidad_annual_to_ahorro',\n  'mrentabilidad_annual_to_sum_fultimo_cierre'],\n 'error with mcuenta_corriente_adicional',\n ['mcuenta_corriente_to_ahorro',\n  'mcuenta_corriente_to_sum_mlimitecompra',\n  'mcuenta_corriente_to_sum_fultimo_cierre',\n  'mcuenta_corriente_to_sum_Fvencimiento',\n  'mcuenta_corriente_to_mrentabilidad'],\n ['mcuentas_saldo_to_sum_fechaalta',\n  'mcuentas_saldo_to_sum_fultimo_cierre',\n  'mcuentas_saldo_to_ahorro'],\n ['mplazo_fijo_dolares_to_sum_mpagospesos',\n  'mplazo_fijo_dolares_to_sum_mfinanciacion_limite',\n  'mplazo_fijo_dolares_to_mcuentas_saldo',\n  'mplazo_fijo_dolares_to_sum_fultimo_cierre',\n  'mplazo_fijo_dolares_to_sum_fechaalta',\n  'mplazo_fijo_dolares_to_tarjeta'],\n ['mplazo_fijo_pesos_to_transfer',\n  'mplazo_fijo_pesos_to_sum_cconsumos',\n  'mplazo_fijo_pesos_to_sum_mconsumototal'],\n ['mpayroll_to_sum_mlimitecompra',\n  'mpayroll_to_sum_fultimo_cierre',\n  'mpayroll_to_ahorro',\n  'mpayroll_to_tarjeta'],\n [],\n ['mpagomiscuentas_to_sum_mfinanciacion_limite',\n  'mpagomiscuentas_to_sum_fultimo_cierre',\n  'mpagomiscuentas_to_sum_fechaalta',\n  'mpagomiscuentas_to_tarjeta'],\n ['mcajeros_propios_descuentos_to_autoservicio',\n  'mcajeros_propios_descuentos_to_sum_mlimitecompra',\n  'mcajeros_propios_descuentos_to_sum_fultimo_cierre',\n  'mcajeros_propios_descuentos_to_mcuentas_saldo',\n  'mcajeros_propios_descuentos_to_ahorro',\n  'mcajeros_propios_descuentos_to_comisiones',\n  'mcajeros_propios_descuentos_to_mrentabilidad'],\n ['sum_delinquency_to_sum_msaldototal',\n  'sum_delinquency_to_sum_fultimo_cierre',\n  'sum_delinquency_to_margen',\n  'sum_delinquency_to_mrentabilidad',\n  'sum_delinquency_to_tarjeta'],\n ['sum_status_to_ahorro',\n  'sum_status_to_sum_fultimo_cierre',\n  'sum_status_to_sum_Fvencimiento',\n  'sum_status_to_tarjeta',\n  'sum_status_to_sum_fechaalta',\n  'sum_status_to_sum_mlimitecompra'],\n ['sum_mfinanciacion_limite_to_ahorro',\n  'sum_mfinanciacion_limite_to_tarjeta',\n  'sum_mfinanciacion_limite_to_sum_fechaalta',\n  'sum_mfinanciacion_limite_to_sum_fultimo_cierre'],\n ['sum_Fvencimiento_to_ahorro', 'sum_Fvencimiento_to_tarjeta'],\n ['sum_msaldototal_to_sum_fultimo_cierre',\n  'sum_msaldototal_to_ahorro',\n  'sum_msaldototal_to_sum_Fvencimiento',\n  'sum_msaldototal_to_sum_fechaalta',\n  'sum_msaldototal_to_tarjeta'],\n ['sum_msaldopesos_to_sum_fultimo_cierre',\n  'sum_msaldopesos_to_ahorro',\n  'sum_msaldopesos_to_sum_Fvencimiento',\n  'sum_msaldopesos_to_sum_fechaalta',\n  'sum_msaldopesos_to_sum_mlimitecompra',\n  'sum_msaldopesos_to_tarjeta'],\n ['sum_msaldodolares_to_sum_msaldototal',\n  'sum_msaldodolares_to_sum_mpagospesos',\n  'sum_msaldodolares_to_sum_fechaalta',\n  'sum_msaldodolares_to_ahorro',\n  'sum_msaldodolares_to_sum_Fvencimiento'],\n ['sum_mconsumospesos_to_sum_fechaalta',\n  'sum_mconsumospesos_to_ahorro',\n  'sum_mconsumospesos_to_sum_mfinanciacion_limite',\n  'sum_mconsumospesos_to_sum_Fvencimiento',\n  'sum_mconsumospesos_to_tarjeta'],\n ['sum_mconsumosdolares_to_sum_mconsumospesos',\n  'sum_mconsumosdolares_to_sum_cconsumos',\n  'sum_mconsumosdolares_to_mcuentas_saldo',\n  'sum_mconsumosdolares_to_ahorro',\n  'sum_mconsumosdolares_to_mrentabilidad',\n  'sum_mconsumosdolares_to_mrentabilidad_annual',\n  'sum_mconsumosdolares_to_sum_mconsumototal',\n  'sum_mconsumosdolares_to_sum_mfinanciacion_limite',\n  'sum_mconsumosdolares_to_sum_msaldototal'],\n ['sum_mlimitecompra_to_ahorro', 'sum_mlimitecompra_to_tarjeta'],\n [],\n 'error with sum_madelantodolares',\n ['sum_fultimo_cierre_to_ahorro', 'sum_fultimo_cierre_to_tarjeta'],\n ['sum_mpagado_to_sum_msaldopesos',\n  'sum_mpagado_to_ahorro',\n  'sum_mpagado_to_sum_Fvencimiento',\n  'sum_mpagado_to_sum_fultimo_cierre',\n  'sum_mpagado_to_sum_fechaalta',\n  'sum_mpagado_to_sum_mlimitecompra'],\n ['sum_mpagospesos_to_sum_fechaalta',\n  'sum_mpagospesos_to_sum_Fvencimiento',\n  'sum_mpagospesos_to_ahorro',\n  'sum_mpagospesos_to_sum_fultimo_cierre',\n  'sum_mpagospesos_to_tarjeta'],\n ['sum_mpagosdolares_to_sum_mconsumospesos',\n  'sum_mpagosdolares_to_mcuentas_saldo',\n  'sum_mpagosdolares_to_ahorro',\n  'sum_mpagosdolares_to_margen',\n  'sum_mpagosdolares_to_sum_cconsumos',\n  'sum_mpagosdolares_to_mrentabilidad',\n  'sum_mpagosdolares_to_sum_fultimo_cierre',\n  'sum_mpagosdolares_to_sum_mconsumototal',\n  'sum_mpagosdolares_to_tarjeta'],\n ['sum_fechaalta_to_ahorro', 'sum_fechaalta_to_tarjeta'],\n ['sum_mconsumototal_to_sum_fechaalta',\n  'sum_mconsumototal_to_ahorro',\n  'sum_mconsumototal_to_sum_mfinanciacion_limite',\n  'sum_mconsumototal_to_sum_Fvencimiento',\n  'sum_mconsumototal_to_tarjeta'],\n ['sum_cconsumos_to_sum_fechaalta',\n  'sum_cconsumos_to_sum_mconsumospesos',\n  'sum_cconsumos_to_ahorro',\n  'sum_cconsumos_to_sum_fultimo_cierre',\n  'sum_cconsumos_to_sum_Fvencimiento',\n  'sum_cconsumos_to_sum_mconsumototal',\n  'sum_cconsumos_to_sum_msaldopesos'],\n [],\n ['sum_mpagominimo_to_sum_fechaalta',\n  'sum_mpagominimo_to_ahorro',\n  'sum_mpagominimo_to_sum_fultimo_cierre',\n  'sum_mpagominimo_to_tarjeta',\n  'sum_mpagominimo_to_sum_Fvencimiento',\n  'sum_mpagominimo_to_sum_mlimitecompra'],\n ['prestamo_to_sum_fechaalta',\n  'prestamo_to_sum_Fvencimiento',\n  'prestamo_to_ahorro',\n  'prestamo_to_sum_fultimo_cierre'],\n ['servicios_to_autoservicio',\n  'servicios_to_sum_cconsumos',\n  'servicios_to_sum_fultimo_cierre',\n  'servicios_to_sum_mconsumospesos',\n  'servicios_to_sum_mpagospesos'],\n ['comisiones_to_ahorro',\n  'comisiones_to_sum_fechaalta',\n  'comisiones_to_sum_mlimitecompra',\n  'comisiones_to_sum_fultimo_cierre',\n  'comisiones_to_mrentabilidad'],\n ['cheques_to_sum_fechaalta',\n  'cheques_to_sum_Fvencimiento',\n  'cheques_to_mcuentas_saldo',\n  'cheques_to_ahorro'],\n ['ahorro_to_sum_fultimo_cierre'],\n ['inversion_to_sum_mfinanciacion_limite',\n  'inversion_to_sum_Fvencimiento',\n  'inversion_to_sum_fultimo_cierre'],\n ['tarjeta_to_sum_fechaalta', 'tarjeta_to_sum_fultimo_cierre'],\n 'error with _consumo',\n ['margen_to_ahorro',\n  'margen_to_sum_mlimitecompra',\n  'margen_to_sum_fultimo_cierre',\n  'margen_to_sum_Fvencimiento'],\n ['debit_to_sum_fechaalta',\n  'debit_to_sum_Fvencimiento',\n  'debit_to_ahorro',\n  'debit_to_tarjeta'],\n ['forex_to_sum_mfinanciacion_limite',\n  'forex_to_sum_Fvencimiento',\n  'forex_to_comisiones',\n  'forex_to_sum_fechaalta',\n  'forex_to_mcuentas_saldo'],\n ['transfer_to_sum_Fvencimiento',\n  'transfer_to_sum_fechaalta',\n  'transfer_to_tarjeta',\n  'transfer_to_ahorro'],\n ['autoservicio_to_sum_Fvencimiento',\n  'autoservicio_to_sum_fultimo_cierre',\n  'autoservicio_to_ahorro',\n  'autoservicio_to_tarjeta'],\n ['atm_to_autoservicio',\n  'atm_to_sum_Fvencimiento',\n  'atm_to_sum_fechaalta',\n  'atm_to_sum_fultimo_cierre',\n  'atm_to_tarjeta',\n  'atm_to_sum_mfinanciacion_limite']]"},"metadata":{}}]},{"cell_type":"code","source":"dtrain = create_ratios_from_best(dtrain, best_unb)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:13:28.150912Z","iopub.execute_input":"2023-04-10T01:13:28.151889Z","iopub.status.idle":"2023-04-10T01:13:28.500888Z","shell.execute_reply.started":"2023-04-10T01:13:28.151834Z","shell.execute_reply":"2023-04-10T01:13:28.499693Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Skipping invalid ratio: error with mcuenta_corriente_adicional\nSkipping invalid ratio: error with sum_madelantodolares\nSkipping invalid ratio: error with _consumo\n","output_type":"stream"}]},{"cell_type":"code","source":"cols_keep_as_is = ['active_quarter','cliente_vip','internet','tcuentas','cdescubierto_preacordado','ccaja_seguridad','tcallcenter','thomebanking','cplazo_fijo','cajas','mobile']\ncols_to_binarize = ['cliente_edad','cliente_antiguedad','cproductos','cpagomiscuentas','ccajeros_propios_descuentos','ccallcenter_transacciones','chomebanking_transacciones','seguro','trx']\ncols_to_normalize = list(dtrain.select_dtypes('float').columns)\n\n#len(cols_keep_as_is + cols_to_binarize+cols_to_normalize) == len(set(cols_keep_as_is + cols_to_binarize+cols_to_normalize))\nset(dtrain.columns) - set(cols_keep_as_is + cols_to_binarize+cols_to_normalize)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:14:22.498802Z","iopub.execute_input":"2023-04-10T01:14:22.499287Z","iopub.status.idle":"2023-04-10T01:14:22.620507Z","shell.execute_reply.started":"2023-04-10T01:14:22.499244Z","shell.execute_reply":"2023-04-10T01:14:22.618974Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'clase_ternaria', 'numero_de_cliente'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# pipeline preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion    \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.validation import check_is_fitted\n\n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.columns]\n\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)\n    \n    \nclass PandasFeatureUnion(BaseEstimator, TransformerMixin):\n    def __init__(self, transformers):\n        self.transformers = transformers\n\n    def fit(self, X, y=None):\n        for _, transformer in self.transformers:\n            transformer.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        transformed_dfs = []\n        for _, transformer in self.transformers:\n            transformed_dfs.append(transformer.transform(X))        \n        \n        concatenated_df = pd.concat(transformed_dfs, axis=1)\n        return concatenated_df\n    \n\nclass BinTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins, columns=None):\n        self.n_bins = n_bins\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        self.quantiles_ = {}\n        self.outliers_ = {}\n        cols_to_transform = self.columns or X.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:          \n            q1, q3 = np.percentile(X[col], [25, 75])\n            iqr = q3 - q1\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n            mask = (X[col] < lower_bound) | (X[col] > upper_bound)\n            self.outliers_[col] = mask\n            col_no_outliers = X.loc[~mask, col]\n            self.quantiles_[col] = pd.qcut(col_no_outliers, self.n_bins, labels=False, duplicates='drop')\n        return self\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X_trans = X.copy()\n        cols_to_transform = self.columns or X_trans.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:         \n            X_trans[col] = pd.cut(X_trans[col], bins=self.n_bins, labels=False, duplicates='drop')\n            X_trans.loc[self.outliers_[col].reindex(X.index, fill_value=False).values, col] = 'outlier'\n            if col in self.quantiles_:\n                X_trans.loc[~self.outliers_[col].reindex(X.index, fill_value=False), col] = 'bin_' + self.quantiles_[col].apply(lambda x: str(x)).astype(str)      \n    \n        return X_trans[cols_to_transform]\n    \n    \nclass OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, categorical_cols=None):\n        self.categorical_cols = categorical_cols\n    \n    def fit(self, X, y=None):\n        if self.categorical_cols is None:\n            self.categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        self.categories_ = [X[col].astype('category').cat.categories.tolist() + ['dummy'] for col in self.categorical_cols]\n        self.encoder = OneHotEncoder(categories=self.categories_, handle_unknown='ignore')\n        self.encoder.fit(X[self.categorical_cols])\n        return self\n    \n    def transform(self, X, y=None):\n        X_transformed = self.encoder.transform(X[self.categorical_cols])\n        feature_names = self.encoder.get_feature_names_out(self.categorical_cols)\n        X_transformed_df = pd.DataFrame.sparse.from_spmatrix(X_transformed, columns=feature_names)\n        X_transformed_df.index = X.index\n        return pd.concat([X.drop(columns=self.categorical_cols), X_transformed_df], axis=1)\n\nimport category_encoders as ce\nclass TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, cols=None, drop_invariant=False, return_df=True):\n        self.cols = cols\n        self.drop_invariant = drop_invariant\n        self.return_df = return_df\n        self.encoder = ce.TargetEncoder(cols=self.cols, drop_invariant=self.drop_invariant, return_df=self.return_df)\n\n    def fit(self, X, y=None):\n        self.encoder.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        return self.encoder.transform(X, y)\n    \n    \n\nfrom sklearn.impute import SimpleImputer\n\nclass SimpleImputerWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=strategy)\n    \n    def fit(self, X, y=None):\n        self.imputer.fit(X)\n        return self\n    \n    def transform(self, X):\n        X_transformed = self.imputer.transform(X)\n        return pd.DataFrame(X_transformed, columns=X.columns, index=X.index)\n    \n    \nclass PercentageVariationTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, variable):\n        self.variable = variable\n\n    def fit(self, X, y):\n        # Combine X and y for easier calculations\n        data = pd.concat([X, y], axis=1)\n\n        # Calculate the mean of the variable for different target values\n        self.target_means = data.groupby(y.name)[self.variable].mean()\n        return self\n\n    def transform(self, X, y=None):\n        result = pd.DataFrame()\n\n        # Calculate the percentage variation for each target mean\n        for target_value, target_mean in self.target_means.items():\n            column_name = f\"{self.variable}_pct_variation_{target_value}\"\n            result[column_name] = (X[self.variable] - target_mean) / target_mean * 100\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:14:48.478399Z","iopub.execute_input":"2023-04-10T01:14:48.478935Z","iopub.status.idle":"2023-04-10T01:14:48.510760Z","shell.execute_reply.started":"2023-04-10T01:14:48.478896Z","shell.execute_reply":"2023-04-10T01:14:48.509597Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## feature engineering","metadata":{}},{"cell_type":"code","source":"class RatioFeature(BaseEstimator, TransformerMixin):\n    def __init__(self, base_var, other_vars):\n        self.base_var = base_var\n        self.other_vars = other_vars if other_vars is not None else []\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Verificar que base_var existe en el DataFrame\n        if self.base_var not in X.columns:\n            raise ValueError(f\"La columna '{self.base_var}' no se encuentra en el DataFrame\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas features\n        new_features = pd.DataFrame()\n\n        # Calcular las nuevas features\n        for var in self.other_vars:\n            if var in X.columns:\n                feature_name = f\"{self.base_var}_{var}_ratio\"\n                new_features[feature_name] = X[self.base_var] / X[var]\n            else:\n                raise ValueError(f\"La columna '{var}' no se encuentra en el DataFrame\")\n        return new_features\n    \n    \nclass JoinColumnsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, like_strings):\n        self.like_strings = like_strings\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas columnas\n        new_cols = pd.DataFrame(index=X.index)\n\n        # Iterar sobre las cadenas especificadas\n        for like_string in self.like_strings:\n            # Filtrar las columnas que contienen la cadena específica\n            joined_cols = X.filter(like=like_string)\n\n            # Crear una nueva columna con la suma de las columnas filtradas\n            new_col_name = 'sum_'+like_string\n            new_cols[new_col_name] = joined_cols.sum(axis=1)\n\n        return new_cols\n    \nfrom sklearn.preprocessing import StandardScaler\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)\n    \n    \nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport pandas as pd\nimport numpy as np\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ReplaceInfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, factor = 3):\n        self.column_max_values = {}\n        self.column_min_values = {}\n        self.factor = factor\n\n    def fit(self, X, y=None):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input should be a pandas DataFrame\")\n\n        for column in X.columns:\n            max_value = X[column].replace([np.inf, -np.inf], np.nan).max()\n            min_value = X[column].replace([np.inf, -np.inf], np.nan).min()\n\n            self.column_max_values[column] = max_value\n            self.column_min_values[column] = min_value\n\n        return self\n\n    def transform(self, X, y=None):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input should be a pandas DataFrame\")\n\n        X_copy = X.copy()\n\n        for column in X_copy.columns:\n            if column in self.column_max_values:\n                X_copy[column] = X_copy[column].replace(np.inf, self.column_max_values[column] * self.factor)\n            if column in self.column_min_values:\n                X_copy[column] = X_copy[column].replace(-np.inf, self.column_min_values[column] * self.factor)\n\n        return X_copy","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:14:49.657029Z","iopub.execute_input":"2023-04-10T01:14:49.657439Z","iopub.status.idle":"2023-04-10T01:14:49.678985Z","shell.execute_reply.started":"2023-04-10T01:14:49.657402Z","shell.execute_reply":"2023-04-10T01:14:49.677789Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"binning = Pipeline([\n    ('col_select',ColumnSelector(columns=cols_to_binarize)),\n    ('bin_convert',BinTransformer(columns=None, n_bins = 3)),\n    ('onehot', OneHotEncoderTransformer())\n    #('target_encoding', TargetEncoderWrapper()) \n])\n\nstandarize = Pipeline([\n    ('col_select',ColumnSelector(columns=cols_to_normalize)),\n    ('scale', StandardScalerTransformer(columns=cols_to_normalize)),\n])\n\nfeature_engineering = PandasFeatureUnion([\n    ('as_is', ColumnSelector(columns=cols_keep_as_is)),\n    ('log', standarize),\n    ('bin_convert',binning)\n])\n\npreprocessing = Pipeline([    \n    ('inf', ReplaceInfTransformer()),\n    ('fillna', SimpleImputerWrapper(strategy='mean')),\n    ('feature_engineering', feature_engineering)\n])","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:19:42.108117Z","iopub.execute_input":"2023-04-10T01:19:42.108527Z","iopub.status.idle":"2023-04-10T01:19:42.116648Z","shell.execute_reply.started":"2023-04-10T01:19:42.108494Z","shell.execute_reply":"2023-04-10T01:19:42.115341Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"X = dtrain.drop('clase_ternaria', axis=1)\ny = dtrain['clase_ternaria']","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:19:42.717688Z","iopub.execute_input":"2023-04-10T01:19:42.718746Z","iopub.status.idle":"2023-04-10T01:19:42.823349Z","shell.execute_reply.started":"2023-04-10T01:19:42.718702Z","shell.execute_reply":"2023-04-10T01:19:42.822201Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the preprocessing pipeline on the training set and transform both the training and test sets\nX_train_processed = preprocessing.fit_transform(X_train, y_train)\nX_test_processed = preprocessing.transform(X_test)\n\n# Check the shapes of the processed data\nprint(X_train_processed.shape, X_test_processed.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:19:43.444707Z","iopub.execute_input":"2023-04-10T01:19:43.445452Z","iopub.status.idle":"2023-04-10T01:19:49.444397Z","shell.execute_reply.started":"2023-04-10T01:19:43.445414Z","shell.execute_reply":"2023-04-10T01:19:49.443135Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"(115277, 274) (49405, 274)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## model evaluation","metadata":{}},{"cell_type":"code","source":"def get_balanced_classes():\n    class_counts = y.value_counts()\n    total_samples = class_counts.sum()\n    class_frequencies = class_counts / total_samples\n    class_weights = 1 / class_frequencies\n    return class_weights.to_dict()\n\nclass_weights_dict = get_balanced_classes()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:19:52.292433Z","iopub.execute_input":"2023-04-10T01:19:52.292908Z","iopub.status.idle":"2023-04-10T01:19:52.303222Z","shell.execute_reply.started":"2023-04-10T01:19:52.292862Z","shell.execute_reply":"2023-04-10T01:19:52.301902Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import early_stopping, log_evaluation\n\ndef objective(trial):\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n\n    f1_scores = []\n    for train_index, val_index in kf.split(X_train_processed):\n        X_tr, X_val = X_train_processed.iloc[train_index], X_train_processed.iloc[val_index]\n        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        params = {\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1.0, log=True),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 128),  # Fix the maximum value for num_leaves\n            \n            'class_weight': class_weights_dict,  \n#             \"objective\": \"multiclass\",\n#             \"metric\": \"multi_logloss\",\n#             \"num_class\": 3,\n#             \"verbosity\": -1,\n#             \"boosting_type\": \"gbdt\",                      \n#             \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),          \n#             \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 50),\n#             \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n#             \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n#             \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n#             \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n#             \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 1e-8, 1.0)\n        }\n\n        clf = lgb.LGBMClassifier(device='gpu', **params)\n        \n        try:\n            clf.fit(\n                X_tr, y_tr, \n                eval_set=[(X_val, y_val)], \n                callbacks=[early_stopping(50), lgb.log_evaluation(period=50)]\n            )    \n        \n        except lgb.basic.LightGBMError as e:\n            print(f\"Trial {trial.number} failed with parameters: {params}\")\n            print(f\"Current best parameters: {trial.study.best_params}\")\n            raise e\n            \n        y_pred = clf.predict(X_val)\n        f1 = f1_score(y_val, y_pred, average='macro')\n        f1_scores.append(f1)\n\n    return sum(f1_scores) / len(f1_scores)\n\n\ndef optimize_lgbm_hyperparameters():\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(f\"Best trial: {study.best_trial.params}\")\n    print(f\"Best F1 score: {study.best_value}\")\n    return study.best_trial.params\n\nbest_params = optimize_lgbm_hyperparameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import validation_curve\n\n# Choose the hyperparameter you want to analyze\nhyperparameter = 'learning_rate'\n\n# Define the range of values for the hyperparameter\nparam_range = np.arange(2, 129, 8)  # Adjust this range as needed\n\n# Get the validation curve\ntrain_scores, test_scores = validation_curve(\n    lgb.LGBMClassifier(device='gpu', **fixed_params),\n    X_train_processed,\n    y_train,\n    param_name=hyperparameter,\n    param_range=param_range,\n    cv=3,\n    scoring='f1_macro',\n    n_jobs=-1\n)\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title(f\"Validation Curve with LightGBM ({hyperparameter})\")\nplt.xlabel(hyperparameter)\nplt.ylabel(\"F1 Score\")\nplt.ylim(0.0, 1.1)\nlw = 2\nplt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\nplt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T01:42:59.470989Z","iopub.execute_input":"2023-04-10T01:42:59.471379Z","iopub.status.idle":"2023-04-10T01:54:03.639876Z","shell.execute_reply.started":"2023-04-10T01:42:59.471344Z","shell.execute_reply":"2023-04-10T01:54:03.638744Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n[LightGBM] [Warning] lambda_l1 is set=0.8104733347910597, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8104733347910597\n[LightGBM] [Warning] min_gain_to_split is set=0.8408350719709347, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.8408350719709347\n[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n[LightGBM] [Warning] feature_fraction is set=0.9878041253717492, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9878041253717492\n[LightGBM] [Warning] lambda_l2 is set=2.1242532204737058e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1242532204737058e-07\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"best_params = {'learning_rate': 0.09148661377934526, 'n_estimators': 819, 'max_depth': 3, 'num_leaves': 92, 'min_data_in_leaf': 32, 'feature_fraction': 0.9878041253717492, 'subsample': 0.4067385074752453, 'lambda_l1': 0.8104733347910597, 'lambda_l2': 2.1242532204737058e-07, 'min_gain_to_split': 0.8408350719709347}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nfrom lightgbm import LGBMClassifier\n\n#class_weight = {0:1, 1:40}\n\nmodel = LGBMClassifier(device='gpu',class_weight = class_weights_dict, **best_params)\n\nmodel.fit(X_train_processed, y_train)\n\ny_pred_proba = model.predict_proba(X_test_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## calculate best threshold & best gain","metadata":{}},{"cell_type":"code","source":"def calculate_gain(cm, fp=-3000, tp=117000):\n    return (cm[0][1] *fp + cm[0][2]*fp + cm[1][2] * fp + (cm[2][2]*tp)+(cm[1][1]*tp))\n\ndef find_best_thresholds(y_test, y_pred_proba, step=0.1):\n    max_gain = float('-inf')\n    best_thresholds = None\n\n    threshold_range = np.arange(0.001, 1 + step, step)\n    for thresholds in itertools.product(threshold_range, repeat=3):\n        y_pred_adjusted_proba = adjust_proba(y_pred_proba, thresholds)\n        y_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n        cm = confusion_matrix(y_test, y_pred_adjusted)\n        gain = calculate_gain(cm)\n\n        if gain > max_gain:\n            max_gain = gain\n            best_thresholds = thresholds\n\n    return best_thresholds, max_gain\n\ndef adjust_proba(proba, thresholds):\n    adjusted_proba = np.zeros_like(proba)\n    for i in range(proba.shape[1]):\n        adjusted_proba[:, i] = proba[:, i] / thresholds[i]\n    return adjusted_proba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_optimal_threshold(model, X_test, y_test, step=0.1, fp=-3000, tp=117000):    \n\n    y_pred_proba = model.predict_proba(X_test)\n    best_thresholds, max_gain = find_best_thresholds(y_test, y_pred_proba, step)\n\n    # Apply the custom threshold function\n    y_pred_adjusted_proba = adjust_proba(y_pred_proba, best_thresholds)\n    y_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n\n    print(\"Best thresholds:\", best_thresholds)\n    print(\"Max gain:\", max_gain)    \n\n    report = classification_report(y_test, y_pred_adjusted)\n    print(\"Classification report:\\n\", report)\n\n    cm = confusion_matrix(y_test, y_pred_adjusted)\n    ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    plt.show()\n    \n    \n    # Calculate SHAP values using TreeExplainer\n    explainer = shap.TreeExplainer(model)\n    X_sample = X_test.sample(1000)\n    shap_values = explainer.shap_values(X_sample)   \n    positive_class_shap_values = shap_values[1]\n    shap.summary_plot(positive_class_shap_values, X_sample, feature_names=X_sample.columns)\n\n    return best_thresholds, y_pred_adjusted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_thresholds, y_pred_adjusted = calculate_optimal_threshold(model, X_train_processed, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_thresholds, y_pred_adjusted = calculate_optimal_threshold(model, X_test_processed, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_train_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred = model.predict(X_test_processed)\n\ncm = confusion_matrix(y_train, y_pred)\nax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# predict and submit","metadata":{}},{"cell_type":"code","source":"dapply = preproc_func(dapply,palabras_agrupar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dapply = create_good_ratios(dapply, best_unb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_result = model.predict_proba(preprocessing.transform(dapply))\n\ny_pred_adjusted_proba = adjust_proba(y_pred_result, best_thresholds)\ny_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n\n# Create a DataFrame for predicted labels using the same index as numero_de_cliente\npredicted_df = pd.DataFrame(y_pred_adjusted, columns=['Predicted'], index=dapply.index)\n\n# Combine the numero_de_cliente and predicted labels\nend_result = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result['Predicted'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#end_result['Predicted'].replace({1:0}, inplace=True)\nend_result['Predicted'].replace({2:1}, inplace=True)\nend_result['Predicted'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using 0.25 thresh\n\ny_pred_result = model.predict_proba(preprocessing.transform(dapply))\npredicted_df = pd.DataFrame((y_pred_result[:,1]>= 0.025).astype(int), columns=['Predicted'], index=dapply.index)\n# Combine the numero_de_cliente and predicted labels\nend_result = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result.to_csv(\"K101_001.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['KAGGLE_USERNAME'] = 'lechuck666'\nos.environ['KAGGLE_KEY'] = '3aaf0a00c8b35504b64d43d4592d6caf'\n!kaggle competitions submit -c laboratorio-de-imp-i-2023-virtual -f ./K101_001.csv -m \"2nd_less var\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}