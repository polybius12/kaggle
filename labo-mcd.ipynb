{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-08T22:54:29.272743Z","iopub.execute_input":"2023-04-08T22:54:29.273122Z","iopub.status.idle":"2023-04-08T22:54:29.287082Z","shell.execute_reply.started":"2023-04-08T22:54:29.273077Z","shell.execute_reply":"2023-04-08T22:54:29.285969Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dset-pequenio/DiccionarioDatos_2023.ods\n/kaggle/input/dset-peq/dataset_pequeno.csv\n/kaggle/input/laboratorio-de-imp-i-2023-virtual/kaggle_competencia_muestra.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport os\n\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy import sparse\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 200)\n\n# Read the dataset\ndataset = pd.read_csv('/kaggle/input/dset-peq/dataset_pequeno.csv')\n\ndtrain = dataset[dataset['foto_mes'] == 202107].copy()\ndapply = dataset[dataset['foto_mes'] == 202109].drop('clase_ternaria', axis=1)\n\ndtrain['clase_ternaria'].replace({'BAJA+2': 2, 'BAJA+1':1, 'CONTINUA':0}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:29.288650Z","iopub.execute_input":"2023-04-08T22:54:29.289016Z","iopub.status.idle":"2023-04-08T22:54:38.911756Z","shell.execute_reply.started":"2023-04-08T22:54:29.288977Z","shell.execute_reply":"2023-04-08T22:54:38.910674Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (154) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"dapply.shape, dtrain.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:42.099374Z","iopub.execute_input":"2023-04-08T22:54:42.100566Z","iopub.status.idle":"2023-04-08T22:54:42.108381Z","shell.execute_reply.started":"2023-04-08T22:54:42.100513Z","shell.execute_reply":"2023-04-08T22:54:42.106599Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"((165237, 154), (164682, 155))"},"metadata":{}}]},{"cell_type":"code","source":"def preproc_func(dtrain, agrup):   \n\n    def find_cols_with_high_nan(df, threshold=0.80): \n        nan_pct = df.isna().sum() / len(df)    \n        # Select columns with NaN percentage greater than the threshold\n        high_nan_cols = nan_pct[nan_pct > threshold].index.tolist()    \n        return high_nan_cols\n\n    def join_cols(df, string):\n        joined_cols = df.filter(like=string)\n        df[string] = joined_cols.sum(axis=1)\n        return dtrain.drop(columns=joined_cols.columns)\n\n    # Get the Visa and Master columns lists\n    visa_columns = dtrain.filter(like='Visa_').columns.to_list()\n    master_columns = dtrain.filter(like='Master_').columns.to_list()\n\n    # Iterate through the corresponding columns and create the new columns in the DataFrame\n    for col_visa, col_master in zip(visa_columns, master_columns):\n        new_column_name = 'sum_' + col_visa.split('_', 1)[1]  # Remove the 'Visa_' prefix and add 'sum_' prefix\n        dtrain[new_column_name] = dtrain[col_visa] + dtrain[col_master]\n\n    dtrain.drop(columns=visa_columns+master_columns, inplace=True)\n\n    for i in agrup:\n        dtrain = join_cols(dtrain, i)\n\n    cols_to_drop =['ccuenta_corriente','foto_mes'] + find_cols_with_high_nan(dtrain)\n    dtrain = dtrain.drop(columns=cols_to_drop)\n\n    dtrain.replace([np.inf], 999999999, inplace=True)\n    dtrain.replace([-np.inf], -999999999, inplace=True)    \n    \n    return dtrain","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:42.412029Z","iopub.execute_input":"2023-04-08T22:54:42.412408Z","iopub.status.idle":"2023-04-08T22:54:42.422820Z","shell.execute_reply.started":"2023-04-08T22:54:42.412374Z","shell.execute_reply":"2023-04-08T22:54:42.421696Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# # Asumiendo que ya tienes un DataFrame llamado 'dtrain'\n\n# # Proporción de rentabilidad mensual y anual\n# dtrain['rentabilidad_ratio'] = dtrain['mrentabilidad'] / dtrain['mrentabilidad_annual']\n\n# # Proporción de comisiones y rentabilidad mensual\n# dtrain['comisiones_rentabilidad_ratio'] = dtrain['comision'] / dtrain['mrentabilidad']\n\n# # Promedio de transacciones por producto\n# dtrain['transacciones_producto_ratio'] = (dtrain['ctarjeta_debito_transacciones'] +\n#                                          dtrain['ctarjeta_visa_transacciones'] +\n#                                          dtrain['ctarjeta_master_transacciones']) / dtrain['cproductos']\n\n# # Promedio de saldo por cuenta\n# dtrain['saldo_cuenta_ratio'] = dtrain['mcuentas_saldo'] / dtrain['tcuentas']\n\n# # Proporción de préstamos personales y prendarios\n# dtrain['prestamos_personales_prendarios_ratio'] = dtrain['cprestamos_personales'] / dtrain['cprestamos_prendarios']\n\n# # Proporción de préstamos hipotecarios y personales\n# dtrain['prestamos_hipotecarios_personales_ratio'] = dtrain['cprestamos_hipotecarios'] / dtrain['cprestamos_personales']\n\n# # Proporción de plazos fijos en pesos y dólares\n# dtrain['plazo_fijo_pesos_dolares_ratio'] = dtrain['mplazo_fijo_pesos'] / dtrain['mplazo_fijo_dolares']\n\n# # Proporción de inversiones tipo 1 y tipo 2\n# dtrain['inversiones_1_2_ratio'] = dtrain['minversion1_pesos'] / dtrain['minversion2']\n\n\n# # Proporción de débitos automáticos en cuentas y tarjetas de crédito\n# dtrain['debitos_cuenta_tarjeta_ratio'] = dtrain['ccuenta_debitos_automaticos'] / (dtrain['ctarjeta_visa_debitos_automaticos'] + dtrain['ctarjeta_master_debitos_automaticos'])\n\n# # Proporción de pagos de servicios y pagos a través de PagoMisCuentas\n# dtrain['pagos_servicios_pagomiscuentas_ratio'] = dtrain['cpagodeservicios'] / dtrain['cpagomiscuentas']\n\n# # Proporción de descuentos en tarjetas de débito y crédito\n# dtrain['descuentos_debito_credito_ratio'] = (dtrain['ctarjeta_visa_descuentos'] + dtrain['ctarjeta_master_descuentos']) / dtrain['ccajeros_propios_descuentos']","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:42.995513Z","iopub.execute_input":"2023-04-08T22:54:42.997448Z","iopub.status.idle":"2023-04-08T22:54:43.003552Z","shell.execute_reply.started":"2023-04-08T22:54:42.997399Z","shell.execute_reply":"2023-04-08T22:54:43.002432Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# #ya estan o estan duplicadas\n# print('ESTAN:',set(cols_keep_as_is + cols_to_normalize + cols_to_binarize + cols_to_drop + turn_to_bool) - set(dtrain.columns))\n\n# #FALTAN ASIGNAR\n# print('\\n\\nFALTA PROCESAR',set(dtrain.columns) - set(cols_keep_as_is + cols_to_normalize + cols_to_binarize + cols_to_drop + turn_to_bool))","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:43.228454Z","iopub.execute_input":"2023-04-08T22:54:43.229155Z","iopub.status.idle":"2023-04-08T22:54:43.233804Z","shell.execute_reply.started":"2023-04-08T22:54:43.229097Z","shell.execute_reply":"2023-04-08T22:54:43.232652Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# pipeline preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion    \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.validation import check_is_fitted\n\n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.columns]\n\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = self.columns or X.select_dtypes(include='number').columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)\n    \n    \nclass PandasFeatureUnion(BaseEstimator, TransformerMixin):\n    def __init__(self, transformers):\n        self.transformers = transformers\n\n    def fit(self, X, y=None):\n        for _, transformer in self.transformers:\n            transformer.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        transformed_dfs = []\n        for _, transformer in self.transformers:\n            transformed_dfs.append(transformer.transform(X))        \n        \n        concatenated_df = pd.concat(transformed_dfs, axis=1)\n        return concatenated_df\n    \n\nclass BinTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins, columns=None):\n        self.n_bins = n_bins\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        self.quantiles_ = {}\n        self.outliers_ = {}\n        cols_to_transform = self.columns or X.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:          \n            q1, q3 = np.percentile(X[col], [25, 75])\n            iqr = q3 - q1\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n            mask = (X[col] < lower_bound) | (X[col] > upper_bound)\n            self.outliers_[col] = mask\n            col_no_outliers = X.loc[~mask, col]\n            self.quantiles_[col] = pd.qcut(col_no_outliers, self.n_bins, labels=False, duplicates='drop')\n        return self\n\n    def transform(self, X):\n        check_is_fitted(self)\n        X_trans = X.copy()\n        cols_to_transform = self.columns or X_trans.select_dtypes(include=np.number).columns\n        for col in cols_to_transform:         \n            X_trans[col] = pd.cut(X_trans[col], bins=self.n_bins, labels=False, duplicates='drop')\n            X_trans.loc[self.outliers_[col].reindex(X.index, fill_value=False).values, col] = 'outlier'\n            if col in self.quantiles_:\n                X_trans.loc[~self.outliers_[col].reindex(X.index, fill_value=False), col] = 'bin_' + self.quantiles_[col].apply(lambda x: str(x)).astype(str)      \n    \n        return X_trans[cols_to_transform]\n    \n    \nclass OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, categorical_cols=None):\n        self.categorical_cols = categorical_cols\n    \n    def fit(self, X, y=None):\n        if self.categorical_cols is None:\n            self.categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        self.categories_ = [X[col].astype('category').cat.categories.tolist() + ['dummy'] for col in self.categorical_cols]\n        self.encoder = OneHotEncoder(categories=self.categories_, handle_unknown='ignore')\n        self.encoder.fit(X[self.categorical_cols])\n        return self\n    \n    def transform(self, X, y=None):\n        X_transformed = self.encoder.transform(X[self.categorical_cols])\n        feature_names = self.encoder.get_feature_names_out(self.categorical_cols)\n        X_transformed_df = pd.DataFrame.sparse.from_spmatrix(X_transformed, columns=feature_names)\n        X_transformed_df.index = X.index\n        return pd.concat([X.drop(columns=self.categorical_cols), X_transformed_df], axis=1)\n\nimport category_encoders as ce\nclass TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, cols=None, drop_invariant=False, return_df=True):\n        self.cols = cols\n        self.drop_invariant = drop_invariant\n        self.return_df = return_df\n        self.encoder = ce.TargetEncoder(cols=self.cols, drop_invariant=self.drop_invariant, return_df=self.return_df)\n\n    def fit(self, X, y=None):\n        self.encoder.fit(X, y)\n        return self\n\n    def transform(self, X, y=None):\n        return self.encoder.transform(X, y)\n    \n    \n\nfrom sklearn.impute import SimpleImputer\n\nclass SimpleImputerWrapper(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=strategy)\n    \n    def fit(self, X, y=None):\n        self.imputer.fit(X)\n        return self\n    \n    def transform(self, X):\n        X_transformed = self.imputer.transform(X)\n        return pd.DataFrame(X_transformed, columns=X.columns, index=X.index)\n    \n    \nclass PercentageVariationTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, variable):\n        self.variable = variable\n\n    def fit(self, X, y):\n        # Combine X and y for easier calculations\n        data = pd.concat([X, y], axis=1)\n\n        # Calculate the mean of the variable for different target values\n        self.target_means = data.groupby(y.name)[self.variable].mean()\n        return self\n\n    def transform(self, X, y=None):\n        result = pd.DataFrame()\n\n        # Calculate the percentage variation for each target mean\n        for target_value, target_mean in self.target_means.items():\n            column_name = f\"{self.variable}_pct_variation_{target_value}\"\n            result[column_name] = (X[self.variable] - target_mean) / target_mean * 100\n\n        return result\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:44.018091Z","iopub.execute_input":"2023-04-08T22:54:44.018583Z","iopub.status.idle":"2023-04-08T22:54:44.108099Z","shell.execute_reply.started":"2023-04-08T22:54:44.018538Z","shell.execute_reply":"2023-04-08T22:54:44.107083Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## feature engineering","metadata":{}},{"cell_type":"code","source":"class RatioFeature(BaseEstimator, TransformerMixin):\n    def __init__(self, base_var, other_vars):\n        self.base_var = base_var\n        self.other_vars = other_vars if other_vars is not None else []\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Verificar que base_var existe en el DataFrame\n        if self.base_var not in X.columns:\n            raise ValueError(f\"La columna '{self.base_var}' no se encuentra en el DataFrame\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas features\n        new_features = pd.DataFrame()\n\n        # Calcular las nuevas features\n        for var in self.other_vars:\n            if var in X.columns:\n                feature_name = f\"{self.base_var}_{var}_ratio\"\n                new_features[feature_name] = X[self.base_var] / X[var]\n            else:\n                raise ValueError(f\"La columna '{var}' no se encuentra en el DataFrame\")\n        return new_features\n    \n    \nclass JoinColumnsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, like_strings):\n        self.like_strings = like_strings\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Asegurarse de que el input es un DataFrame de pandas\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"El input debe ser un DataFrame de pandas\")\n\n        # Crear un DataFrame vacío para almacenar las nuevas columnas\n        new_cols = pd.DataFrame(index=X.index)\n\n        # Iterar sobre las cadenas especificadas\n        for like_string in self.like_strings:\n            # Filtrar las columnas que contienen la cadena específica\n            joined_cols = X.filter(like=like_string)\n\n            # Crear una nueva columna con la suma de las columnas filtradas\n            new_col_name = 'sum_'+like_string\n            new_cols[new_col_name] = joined_cols.sum(axis=1)\n\n        return new_cols\n    \nfrom sklearn.preprocessing import StandardScaler\n\nclass StandardScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        self.scaler.fit(X[cols_to_scale])\n        return self\n        \n    def transform(self, X):\n        cols_to_scale = X.select_dtypes(include='number').columns if self.columns is None else self.columns\n        X_transformed = pd.DataFrame(self.scaler.transform(X[cols_to_scale]), columns=cols_to_scale, index=X.index)\n        return pd.concat([X.drop(columns=cols_to_scale), X_transformed], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:44.643269Z","iopub.execute_input":"2023-04-08T22:54:44.643645Z","iopub.status.idle":"2023-04-08T22:54:44.657456Z","shell.execute_reply.started":"2023-04-08T22:54:44.643610Z","shell.execute_reply":"2023-04-08T22:54:44.656062Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"palabras_agrupar =['prestamo','seguro','servicios', 'comisiones','cheques','ahorro','inversion','tarjeta','_consumo','margen','debit','forex', 'transfer','autoservicio','cajas','atm','trx','mobile']\n\ndtrain = preproc_func(dtrain,palabras_agrupar)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:45.297601Z","iopub.execute_input":"2023-04-08T22:54:45.300271Z","iopub.status.idle":"2023-04-08T22:54:47.060918Z","shell.execute_reply.started":"2023-04-08T22:54:45.300229Z","shell.execute_reply":"2023-04-08T22:54:47.059831Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"cols_keep_as_is = ['active_quarter','cliente_vip','internet','tcuentas','cdescubierto_preacordado','ccaja_seguridad','tcallcenter','thomebanking','cplazo_fijo','cajas','mobile']\ncols_to_binarize = ['cliente_edad','cliente_antiguedad','cproductos','cpagomiscuentas','ccajeros_propios_descuentos','ccallcenter_transacciones','chomebanking_transacciones','seguro','trx']\ncols_to_normalize = list(dtrain.select_dtypes('float').columns)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:47.064912Z","iopub.execute_input":"2023-04-08T22:54:47.065231Z","iopub.status.idle":"2023-04-08T22:54:47.086261Z","shell.execute_reply.started":"2023-04-08T22:54:47.065202Z","shell.execute_reply":"2023-04-08T22:54:47.085100Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#len(cols_keep_as_is + cols_to_binarize+cols_to_normalize) == len(set(cols_keep_as_is + cols_to_binarize+cols_to_normalize))\nset(dtrain.columns) - set(cols_keep_as_is + cols_to_binarize+cols_to_normalize)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:47.087785Z","iopub.execute_input":"2023-04-08T22:54:47.088444Z","iopub.status.idle":"2023-04-08T22:54:47.099051Z","shell.execute_reply.started":"2023-04-08T22:54:47.088400Z","shell.execute_reply":"2023-04-08T22:54:47.097926Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'clase_ternaria', 'numero_de_cliente'}"},"metadata":{}}]},{"cell_type":"code","source":"binning = Pipeline([\n    ('col_select',ColumnSelector(columns=cols_to_binarize)),\n    ('bin_convert',BinTransformer(columns=None, n_bins = 6)),\n    ('target_encoding', TargetEncoderWrapper()) \n])\n\nstandarize = Pipeline([\n    ('col_select',ColumnSelector(columns=cols_to_normalize)),\n    ('scale', StandardScalerTransformer(columns=cols_to_normalize)),\n])\n\nfeature_engineering = PandasFeatureUnion([\n    ('as_is', ColumnSelector(columns=cols_keep_as_is)),\n    ('log', standarize),\n    ('bin_convert',binning)\n])\n\npreprocessing = Pipeline([\n    ('fillna', SimpleImputerWrapper(strategy='mean')),\n    ('feature_engineering', feature_engineering)\n])","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:47.767394Z","iopub.execute_input":"2023-04-08T22:54:47.768085Z","iopub.status.idle":"2023-04-08T22:54:47.775414Z","shell.execute_reply.started":"2023-04-08T22:54:47.768045Z","shell.execute_reply":"2023-04-08T22:54:47.774061Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X = dtrain.drop('clase_ternaria', axis=1)\ny = dtrain['clase_ternaria']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the preprocessing pipeline on the training set and transform both the training and test sets\nX_train_processed = preprocessing.fit_transform(X_train, y_train)\nX_test_processed = preprocessing.transform(X_test)\n\n# Check the shapes of the processed data\nprint(X_train_processed.shape, X_test_processed.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:50.851951Z","iopub.execute_input":"2023-04-08T22:54:50.854619Z","iopub.status.idle":"2023-04-08T22:54:55.329766Z","shell.execute_reply.started":"2023-04-08T22:54:50.854577Z","shell.execute_reply":"2023-04-08T22:54:55.328599Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(131745, 66) (32937, 66)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## model evaluation","metadata":{}},{"cell_type":"code","source":"def get_balanced_classes():\n    class_counts = y.value_counts()\n    total_samples = class_counts.sum()\n    class_frequencies = class_counts / total_samples\n    class_weights = 1 / class_frequencies\n    return class_weights.to_dict()\n\nclass_weights_dict = get_balanced_classes()","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:54:55.331652Z","iopub.execute_input":"2023-04-08T22:54:55.332613Z","iopub.status.idle":"2023-04-08T22:54:55.340536Z","shell.execute_reply.started":"2023-04-08T22:54:55.332572Z","shell.execute_reply":"2023-04-08T22:54:55.339407Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import early_stopping, log_evaluation\n\ndef objective(trial):\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n\n    f1_scores = []\n    for train_index, val_index in kf.split(X_train_processed):\n        X_tr, X_val = X_train_processed.iloc[train_index], X_train_processed.iloc[val_index]\n        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        \n        max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n        num_leaves_upper_limit = min(256, 2 ** (max_depth - 1))\n        if num_leaves_upper_limit < 2:\n            num_leaves_upper_limit = 2\n\n        params = {\n        \"objective\": \"multiclass\",\n        \"metric\": \"multi_logloss\",\n        \"num_class\": 3,\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        'class_weight': class_weights_dict,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1.0, log=True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, num_leaves_upper_limit),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 50),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 1e-8, 1.0)\n        }\n\n        clf = lgb.LGBMClassifier(device='gpu', **params)\n        \n        try:\n            clf.fit(\n                X_tr, y_tr, \n                eval_set=[(X_val, y_val)], \n                callbacks=[early_stopping(50), lgb.log_evaluation(period=50)]\n            )    \n        \n        except lgb.basic.LightGBMError as e:\n            print(f\"Trial {trial.number} failed with parameters: {params}\")\n            print(f\"Current best parameters: {trial.study.best_params}\")\n            raise e\n            \n        y_pred = clf.predict(X_val)\n        f1 = f1_score(y_val, y_pred, average='macro')\n        f1_scores.append(f1)\n\n    return sum(f1_scores) / len(f1_scores)\n\ndef optimize_lgbm_hyperparameters():\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(f\"Best trial: {study.best_trial.params}\")\n    print(f\"Best F1 score: {study.best_value}\")\n    return study.best_trial.params\n\nbest_params = optimize_lgbm_hyperparameters()","metadata":{"execution":{"iopub.status.busy":"2023-04-08T22:55:01.295974Z","iopub.execute_input":"2023-04-08T22:55:01.297175Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[32m[I 2023-04-08 22:55:04,344]\u001b[0m A new study created in memory with name: no-name-d9007f19-1db9-42c5-9634-eb383c3e1fcb\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] lambda_l1 is set=0.0007452861197557821, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007452861197557821\n[LightGBM] [Warning] min_gain_to_split is set=0.9062902304742989, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.9062902304742989\n[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n[LightGBM] [Warning] feature_fraction is set=0.653759409017161, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.653759409017161\n[LightGBM] [Warning] lambda_l2 is set=0.011243896654581925, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011243896654581925\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.139951\n[100]\tvalid_0's multi_logloss: 0.0870186\n[150]\tvalid_0's multi_logloss: 0.0833859\nEarly stopping, best iteration is:\n[122]\tvalid_0's multi_logloss: 0.0833859\n[LightGBM] [Warning] lambda_l1 is set=0.0007452861197557821, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007452861197557821\n[LightGBM] [Warning] min_gain_to_split is set=0.9062902304742989, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.9062902304742989\n[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n[LightGBM] [Warning] feature_fraction is set=0.653759409017161, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.653759409017161\n[LightGBM] [Warning] lambda_l2 is set=0.011243896654581925, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011243896654581925\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.148199\n[100]\tvalid_0's multi_logloss: 0.0896791\n[150]\tvalid_0's multi_logloss: 0.0852108\nEarly stopping, best iteration is:\n[132]\tvalid_0's multi_logloss: 0.0852108\n[LightGBM] [Warning] lambda_l1 is set=0.0007452861197557821, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007452861197557821\n[LightGBM] [Warning] min_gain_to_split is set=0.9062902304742989, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.9062902304742989\n[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n[LightGBM] [Warning] feature_fraction is set=0.653759409017161, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.653759409017161\n[LightGBM] [Warning] lambda_l2 is set=0.011243896654581925, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011243896654581925\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.148516\n[100]\tvalid_0's multi_logloss: 0.0936166\n[150]\tvalid_0's multi_logloss: 0.0870847\nEarly stopping, best iteration is:\n[135]\tvalid_0's multi_logloss: 0.0870847\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2023-04-08 22:55:44,536]\u001b[0m Trial 0 finished with value: 0.4308012029129311 and parameters: {'max_depth': 10, 'learning_rate': 0.15841455799908702, 'n_estimators': 693, 'num_leaves': 114, 'min_data_in_leaf': 38, 'feature_fraction': 0.653759409017161, 'subsample': 0.5689462731600926, 'lambda_l1': 0.0007452861197557821, 'lambda_l2': 0.011243896654581925, 'min_gain_to_split': 0.9062902304742989}. Best is trial 0 with value: 0.4308012029129311.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] lambda_l1 is set=5.505447564314054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.505447564314054e-08\n[LightGBM] [Warning] min_gain_to_split is set=0.0924830855105328, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0924830855105328\n[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n[LightGBM] [Warning] feature_fraction is set=0.9800769179632726, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800769179632726\n[LightGBM] [Warning] lambda_l2 is set=1.110997321030435e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.110997321030435e-08\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.319919\n[100]\tvalid_0's multi_logloss: 0.21236\n[150]\tvalid_0's multi_logloss: 0.158977\n[200]\tvalid_0's multi_logloss: 0.128507\n[250]\tvalid_0's multi_logloss: 0.111133\n[300]\tvalid_0's multi_logloss: 0.101976\n[350]\tvalid_0's multi_logloss: 0.0974878\n[400]\tvalid_0's multi_logloss: 0.0963153\nEarly stopping, best iteration is:\n[397]\tvalid_0's multi_logloss: 0.0960884\n[LightGBM] [Warning] lambda_l1 is set=5.505447564314054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.505447564314054e-08\n[LightGBM] [Warning] min_gain_to_split is set=0.0924830855105328, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0924830855105328\n[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n[LightGBM] [Warning] feature_fraction is set=0.9800769179632726, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800769179632726\n[LightGBM] [Warning] lambda_l2 is set=1.110997321030435e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.110997321030435e-08\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.323079\n[100]\tvalid_0's multi_logloss: 0.221847\n[150]\tvalid_0's multi_logloss: 0.168231\n[200]\tvalid_0's multi_logloss: 0.136002\n[250]\tvalid_0's multi_logloss: 0.117018\n[300]\tvalid_0's multi_logloss: 0.10716\n[350]\tvalid_0's multi_logloss: 0.102708\n[400]\tvalid_0's multi_logloss: 0.100341\n[450]\tvalid_0's multi_logloss: 0.100195\nEarly stopping, best iteration is:\n[417]\tvalid_0's multi_logloss: 0.0998667\n[LightGBM] [Warning] lambda_l1 is set=5.505447564314054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.505447564314054e-08\n[LightGBM] [Warning] min_gain_to_split is set=0.0924830855105328, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0924830855105328\n[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n[LightGBM] [Warning] feature_fraction is set=0.9800769179632726, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800769179632726\n[LightGBM] [Warning] lambda_l2 is set=1.110997321030435e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.110997321030435e-08\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.308119\n[100]\tvalid_0's multi_logloss: 0.208892\n[150]\tvalid_0's multi_logloss: 0.159019\n[200]\tvalid_0's multi_logloss: 0.131496\n[250]\tvalid_0's multi_logloss: 0.114213\n[300]\tvalid_0's multi_logloss: 0.105644\n[350]\tvalid_0's multi_logloss: 0.100956\n[400]\tvalid_0's multi_logloss: 0.0994846\nEarly stopping, best iteration is:\n[390]\tvalid_0's multi_logloss: 0.0991992\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2023-04-08 22:56:33,748]\u001b[0m Trial 1 finished with value: 0.4230773366022209 and parameters: {'max_depth': 5, 'learning_rate': 0.3346359549838362, 'n_estimators': 491, 'num_leaves': 15, 'min_data_in_leaf': 16, 'feature_fraction': 0.9800769179632726, 'subsample': 0.6164834235845331, 'lambda_l1': 5.505447564314054e-08, 'lambda_l2': 1.110997321030435e-08, 'min_gain_to_split': 0.0924830855105328}. Best is trial 0 with value: 0.4308012029129311.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] lambda_l1 is set=0.18525683187199513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18525683187199513\n[LightGBM] [Warning] min_gain_to_split is set=0.6349329149454785, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.6349329149454785\n[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n[LightGBM] [Warning] feature_fraction is set=0.659571150568529, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.659571150568529\n[LightGBM] [Warning] lambda_l2 is set=3.475707316759221e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.475707316759221e-05\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.912097\n[100]\tvalid_0's multi_logloss: 0.783366\n[150]\tvalid_0's multi_logloss: 0.682769\n[200]\tvalid_0's multi_logloss: 0.603436\n[250]\tvalid_0's multi_logloss: 0.539853\n[300]\tvalid_0's multi_logloss: 0.488287\n[350]\tvalid_0's multi_logloss: 0.445947\n[400]\tvalid_0's multi_logloss: 0.410727\n[450]\tvalid_0's multi_logloss: 0.381153\n[500]\tvalid_0's multi_logloss: 0.356344\n[550]\tvalid_0's multi_logloss: 0.335283\n[600]\tvalid_0's multi_logloss: 0.317544\n[650]\tvalid_0's multi_logloss: 0.302098\nDid not meet early stopping. Best iteration is:\n[666]\tvalid_0's multi_logloss: 0.29764\n[LightGBM] [Warning] lambda_l1 is set=0.18525683187199513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18525683187199513\n[LightGBM] [Warning] min_gain_to_split is set=0.6349329149454785, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.6349329149454785\n[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n[LightGBM] [Warning] feature_fraction is set=0.659571150568529, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.659571150568529\n[LightGBM] [Warning] lambda_l2 is set=3.475707316759221e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.475707316759221e-05\nTraining until validation scores don't improve for 50 rounds\n[50]\tvalid_0's multi_logloss: 0.931256\n[100]\tvalid_0's multi_logloss: 0.799567\n[150]\tvalid_0's multi_logloss: 0.697054\n[200]\tvalid_0's multi_logloss: 0.616518\n[250]\tvalid_0's multi_logloss: 0.551957\n[300]\tvalid_0's multi_logloss: 0.49973\n[350]\tvalid_0's multi_logloss: 0.456547\n[400]\tvalid_0's multi_logloss: 0.421213\n[450]\tvalid_0's multi_logloss: 0.391544\n[500]\tvalid_0's multi_logloss: 0.36705\n[550]\tvalid_0's multi_logloss: 0.346687\n[600]\tvalid_0's multi_logloss: 0.329443\n[650]\tvalid_0's multi_logloss: 0.314287\nDid not meet early stopping. Best iteration is:\n[666]\tvalid_0's multi_logloss: 0.309909\n","output_type":"stream"}]},{"cell_type":"code","source":"best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nfrom lightgbm import LGBMClassifier\n\n#class_weight = {0:1, 1:40, 2:80} (best score)\n\nmodel = LGBMClassifier(device='gpu',class_weight = class_weights_dict, **best_params)\n\nmodel.fit(X_train_processed, y_train)\n\ny_pred_proba = model.predict_proba(X_test_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## calculate best threshold & best gain","metadata":{}},{"cell_type":"code","source":"def calculate_gain(cm, fp=-3000, tp=117000):\n    return (cm[0][1] *fp + cm[0][2]*fp + cm[1][2] * fp + (cm[2][2]*tp)+(cm[1][1]*tp))\n\ndef find_best_thresholds(y_test, y_pred_proba, step=0.1):\n    max_gain = float('-inf')\n    best_thresholds = None\n\n    threshold_range = np.arange(0.001, 1 + step, step)\n    for thresholds in itertools.product(threshold_range, repeat=3):\n        y_pred_adjusted_proba = adjust_proba(y_pred_proba, thresholds)\n        y_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n        cm = confusion_matrix(y_test, y_pred_adjusted)\n        gain = calculate_gain(cm)\n\n        if gain > max_gain:\n            max_gain = gain\n            best_thresholds = thresholds\n\n    return best_thresholds, max_gain\n\ndef adjust_proba(proba, thresholds):\n    adjusted_proba = np.zeros_like(proba)\n    for i in range(proba.shape[1]):\n        adjusted_proba[:, i] = proba[:, i] / thresholds[i]\n    return adjusted_proba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_optimal_threshold(model, X_test, y_test, step=0.1, fp=-3000, tp=117000):    \n\n    y_pred_proba = model.predict_proba(X_test)\n    best_thresholds, max_gain = find_best_thresholds(y_test, y_pred_proba, step)\n\n    # Apply the custom threshold function\n    y_pred_adjusted_proba = adjust_proba(y_pred_proba, best_thresholds)\n    y_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n\n    print(\"Best thresholds:\", best_thresholds)\n    print(\"Max gain:\", max_gain)    \n\n    report = classification_report(y_test, y_pred_adjusted)\n    print(\"Classification report:\\n\", report)\n\n    cm = confusion_matrix(y_test, y_pred_adjusted)\n    ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    plt.show()\n    \n    \n    # Calculate SHAP values using TreeExplainer\n    explainer = shap.TreeExplainer(model)\n    X_sample = X_test.sample(1000)\n    shap_values = explainer.shap_values(X_sample)   \n    positive_class_shap_values = shap_values[1]\n    shap.summary_plot(positive_class_shap_values, X_sample, feature_names=X_sample.columns)\n\n    return best_thresholds, y_pred_adjusted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_thresholds, y_pred_adjusted = calculate_optimal_threshold(model, X_train_processed, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_thresholds, y_pred_adjusted = calculate_optimal_threshold(model, X_test_processed, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# predict and submit","metadata":{}},{"cell_type":"code","source":"dapply = preproc_func(dapply,palabras_agrupar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_result = model.predict_proba(preprocessing.transform(dapply))\n\ny_pred_adjusted_proba = adjust_proba(y_pred_result, best_thresholds)\ny_pred_adjusted = np.argmax(y_pred_adjusted_proba, axis=1)\n\n# Create a DataFrame for predicted labels using the same index as numero_de_cliente\npredicted_df = pd.DataFrame(y_pred_adjusted, columns=['Predicted'], index=dapply.index)\n\n# Combine the numero_de_cliente and predicted labels\nend_result = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result['Predicted'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using 0.25 thresh\n\ny_pred_result = model.predict_proba(preprocessing.transform(dapply))\npredicted_df = pd.DataFrame((y_pred_result[:,1]>= 0.025).astype(int), columns=['Predicted'], index=dapply.index)\n# Combine the numero_de_cliente and predicted labels\nend_result = pd.concat([dapply['numero_de_cliente'], predicted_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result['Predicted'].replace({1:0}, inplace=True)\nend_result['Predicted'].replace({2:1}, inplace=True)\nend_result['Predicted'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_result.to_csv(\"K101_001.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['KAGGLE_USERNAME'] = 'lechuck666'\nos.environ['KAGGLE_KEY'] = '3aaf0a00c8b35504b64d43d4592d6caf'\n!kaggle competitions submit -c laboratorio-de-imp-i-2023-virtual -f ./K101_001.csv -m \"2nd_less var\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}